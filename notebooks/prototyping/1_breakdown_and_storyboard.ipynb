{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2217fec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AIz...8\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pathlib\n",
    "import pickle\n",
    "\n",
    "import httpx\n",
    "from dotenv import load_dotenv\n",
    "from google import genai\n",
    "from google.genai import types as gemini_types\n",
    "\n",
    "load_dotenv(dotenv_path=\"../../.env\")\n",
    "\n",
    "aistudio_gemini_api_key = os.environ['GOOGLE_API_KEY']\n",
    "print(aistudio_gemini_api_key[:3] + '...' + aistudio_gemini_api_key[-1:])\n",
    "gemini_client = genai.Client(api_key=aistudio_gemini_api_key)\n",
    "\n",
    "MODEL_NAME = \"gemini-3-flash-preview\"\n",
    "\n",
    "\n",
    "# MODEL_NAME = \"gemini-3-pro-preview\"\n",
    "# vertex_gemini_key = os.environ[\"VERTEX_AI_KEY\"]\n",
    "# headers = {\"Authorization\": f\"Basic {vertex_gemini_key}\"}\n",
    "# base_url = \"https://generative-ai-proxy.rcp.us-east-1.data.test.exp-aws.net/v1/proxy/vertex-ai\"\n",
    "\n",
    "# http_options=gemini_types.HttpOptions(\n",
    "#         headers=headers,\n",
    "#         api_version=\"\",\n",
    "#         base_url=base_url,\n",
    "#         httpx_client=httpx.Client(verify=False)\n",
    "#     )\n",
    "\n",
    "\n",
    "# gemini_client = genai.Client(\n",
    "#     api_key=\"dummy_value_to_by_pass_validation\",\n",
    "#     http_options=http_options\n",
    "# )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d62a28fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from eduly import EdulyClient\n",
    "\n",
    "eduly_client = EdulyClient(gemini_client=gemini_client)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad68a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # takes ~30 seconds\n",
    "# breakdown_obj, raw_breakdown_response = eduly_client.breakdown(\n",
    "#     file_path=pathlib.Path(\"./deepseek_mhc/deepseek_mhc.pdf\"),\n",
    "#     model=MODEL_NAME,\n",
    "#     thinking_level=\"high\"\n",
    "# )\n",
    "\n",
    "# with open('./deepseek_mhc/deepseek_mhc_breakdown.pkl', 'wb') as f:\n",
    "#     pickle.dump(raw_breakdown_response, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8a72b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from eduly.models import Breakdown\n",
    "\n",
    "with open('./deepseek_mhc/deepseek_mhc_breakdown.pkl', 'rb') as f:\n",
    "    raw_breakdown_response = pickle.load(f)\n",
    "\n",
    "breakdown_obj = Breakdown.model_validate_json(raw_breakdown_response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78afa449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "**name**: The Evolution of Residual Connections\n",
      "\n",
      "**summary**: For nearly a decade, the Residual Connection has been the backbone of deep learning, but modern models are pushing its limits. This topic explores why the classic identity mapping is so critical for training stability in models like Transformers.\n",
      "\n",
      "**full_explanation**:\n",
      "Since the introduction of **ResNets** in 2016, the formula $x_{l+1} = x_l + \\mathcal{F}(x_l, W_l)$ has dominated neural architecture. In this equation, $x_l$ represents the input to the $l$-th layer, and $\\mathcal{F}$ is the residual function (like an **Attention** or **FFN** block). The term $x_l$ on the right side is the **identity mapping**, which allows signals to pass through the network without modification. As shown in Figure 1(a), this simple addition ensures that the gradient can flow back to early layers during training, preventing the 'vanishing gradient' problem. However, as we move into the era of massive **Large Language Models (LLMs)**, researchers are looking for ways to make this connection more expressive without losing the stability that the identity mapping provides. While standard Transformers have become the industry standard, this paper argues that the way we connect layers (the **Macro-Design**) is overdue for an upgrade beyond simple addition.\n",
      "\n",
      "**key_takeaways**:\n",
      "- Identity mapping ($x_l$) allows signal conservation across hundreds of layers.\n",
      "- Classic residual connections are the foundation of modern LLM architectures like GPT and Llama.\n",
      "- Stability in large-scale training depends on the gradient's ability to flow unimpeded.\n",
      "--------------------------------\n",
      "--------------------------------\n",
      "**name**: Hyper-Connections (HC): Expanding the Residual Stream\n",
      "\n",
      "**summary**: What if the 'highway' between layers was wider? Hyper-Connections (HC) attempt to boost model capacity by expanding the residual stream width, though it introduces a new risk: instability.\n",
      "\n",
      "**full_explanation**:\n",
      "The concept of **Hyper-Connections (HC)**, introduced by Zhu et al. in 2024, moves beyond the single-stream residual connection. Instead of a single vector $x_l$, HC expands the feature dimension by an expansion rate $n$ (e.g., $n=4$), creating a multi-stream residual path. As seen in Figure 1(b), the layer propagation is redefined as $x_{l+1} = \\mathcal{H}_l^{res} x_l + \\mathcal{H}_l^{post \\top} \\mathcal{F}(\\mathcal{H}_l^{pre} x_l, W_l)$. Here, $\\mathcal{H}_l^{res}$ is a learnable matrix that mixes features across the widened stream, while $\\mathcal{H}_l^{pre}$ and $\\mathcal{H}_l^{post}$ handle the 'read' and 'write' operations between the stream and the actual computation layer $\\mathcal{F}$. While this significantly increases the model's **topological complexity** and performance potential without adding significant **FLOPs** (Floating Point Operations), it creates a 'identity crisis.' Because the matrix $\\mathcal{H}_l^{res}$ is unconstrained, it can amplify or attenuate signals as they pass through multiple layers, leading to numerical explosion or vanishing.\n",
      "\n",
      "**key_takeaways**:\n",
      "- HC expands the residual stream width by a factor of $n$ to increase model capacity.\n",
      "- It uses learnable mappings ($\\mathcal{H}^{res}$) to mix features between parallel streams.\n",
      "- Unconstrained learnable connections risk breaking the stability of the identity mapping.\n",
      "--------------------------------\n",
      "--------------------------------\n",
      "**name**: The Stability Crisis: Signal Explosion in HC\n",
      "\n",
      "**summary**: When you give a neural network too much freedom in how it connects layers, the math can literally blow up. This topic breaks down why unconstrained Hyper-Connections fail at scale.\n",
      "\n",
      "**full_explanation**:\n",
      "The primary weakness of standard **Hyper-Connections (HC)** is that they lack the 'conservation mechanism' of the identity mapping. In a traditional ResNet, the signal mean and variance stay relatively stable. In HC, when you chain many layers together, the composite mapping becomes a product of matrices: $\\prod_{i=1}^{L-l} \\mathcal{H}_{L-i}^{res}$. Since these matrices are learnable and unconstrained, their **spectral norm** can easily exceed 1. Figure 3(b) in the paper illustrates this 'Propagation Instability,' showing that the **Amax Gain Magnitude** (the maximum expansion of a signal) can peak at values as high as 3000 in a 27-billion parameter model. This leads to massive spikes in **gradient norms** (shown in Figure 2b) and sudden training 'loss surges.' Essentially, the 'highway' of the model becomes an echo chamber where signals are amplified until they become noise, making it impossible to train very deep or large-scale models reliably.\n",
      "\n",
      "**key_takeaways**:\n",
      "- Unconstrained matrices in HC lead to unbounded signal amplification.\n",
      "- Amax Gain Magnitude measures how much a signal is expanded; HC can reach values of 3000x.\n",
      "- Instability manifests as loss surges and exploding gradient norms during training.\n",
      "--------------------------------\n",
      "--------------------------------\n",
      "**name**: mHC: The Manifold-Constrained Solution\n",
      "\n",
      "**summary**: To fix Hyper-Connections, mHC forces the connection matrices to stay within a 'safe zone.' This is done by projecting them onto a specific mathematical manifold.\n",
      "\n",
      "**full_explanation**:\n",
      "The authors propose **Manifold-Constrained Hyper-Connections (mHC)** to restore stability. The core idea, illustrated in Figure 1(c), is to constrain the residual mapping $\\mathcal{H}_l^{res}$ to a specific mathematical space called the **Birkhoff polytope**. This is the manifold of all **doubly stochastic matrices**. A doubly stochastic matrix has two key properties: every entry is non-negative, and every row and column sums to exactly 1. Mathematically, this is expressed as $\\mathcal{H}_l^{res} \\mathbf{1}_n = \\mathbf{1}_n$ and $\\mathbf{1}_n^\\top \\mathcal{H}_l^{res} = \\mathbf{1}_n^\\top$. When a matrix is doubly stochastic, its **spectral norm** is bounded by 1, making it a 'non-expansive' mapping. By forcing the learnable connections to stay on this manifold, mHC ensures that the total energy of the signal is conserved across the entire depth of the network, effectively acting as a 'convex combination' of features that prevents explosion while still allowing complex information exchange between residual streams.\n",
      "\n",
      "**key_takeaways**:\n",
      "- mHC constrains the connection matrix to the Birkhoff polytope (doubly stochastic matrices).\n",
      "- Doubly stochastic matrices ensure row and column sums equal 1, preserving feature means.\n",
      "- This projection restores the 'identity mapping' property while keeping the benefits of wide streams.\n",
      "--------------------------------\n",
      "--------------------------------\n",
      "**name**: The Sinkhorn-Knopp Algorithm: Enforcing Constraints\n",
      "\n",
      "**summary**: How do you actually force a random matrix to become doubly stochastic? You use the Sinkhorn-Knopp algorithm, a classic mathematical tool adapted for modern deep learning.\n",
      "\n",
      "**full_explanation**:\n",
      "To project the learnable matrices onto the doubly stochastic manifold, mHC uses the **Sinkhorn-Knopp algorithm**. The process starts with a matrix $\\mathbf{M}^{(0)} = \\exp(\\tilde{\\mathcal{H}}_l^{res})$, where $\\tilde{\\mathcal{H}}_l^{res}$ is the raw output of a linear layer. The algorithm then performs an iterative normalization process: it alternates between rescaling the rows so they sum to 1 and rescaling the columns so they sum to 1. As shown in Equation 9: $\\mathbf{M}^{(t)} = \\mathcal{T}_r(\\mathcal{T}_c(\\mathbf{M}^{(t-1)}))$. The authors found that $t_{max} = 20$ iterations is sufficient for convergence. Since the set of doubly stochastic matrices is **closed under matrix multiplication**, the entire chain of connections across the model remains stable. Modern evolution of this technique often involves 'Gumbel-Sinkhorn' variants for discrete routing, but here it is used as a continuous, differentiable constraint to ensure structural stability in dense connections.\n",
      "\n",
      "**key_takeaways**:\n",
      "- Sinkhorn-Knopp iteratively rescales rows and columns to sum to 1.\n",
      "- The authors use 20 iterations to achieve a stable, approximate solution.\n",
      "- This process is fully differentiable, allowing the model to learn the optimal 'mixing' within the constraint.\n",
      "--------------------------------\n",
      "--------------------------------\n",
      "**name**: Breaking the Memory Wall: Infrastructure Optimization\n",
      "\n",
      "**summary**: Expanding the residual stream increases memory traffic, which can slow down training. mHC solves this through 'Kernel Fusion'—combining multiple steps into a single GPU operation.\n",
      "\n",
      "**full_explanation**:\n",
      "Expanding the residual stream by a factor $n$ (like $n=4$) naturally increases memory access costs, often referred to as the **'Memory Wall.'** As shown in Table 2, a standard residual connection requires only $3C$ memory elements (read/write), but HC/mHC can require over $(8n)C$. To prevent this from slowing down training, the authors implemented specialized **GPU kernels** using **TileLang**. They use **Kernel Fusion** to consolidate multiple operations—like RMSNorm, the Sinkhorn-Knopp iterations, and the residual merge—into unified compute kernels. For example, instead of reading and writing data back to the global memory after every small step, the fused kernel keeps the data in the GPU's high-speed 'shared memory.' This optimization allows mHC with $n=4$ to run with only a marginal **6.7% training time overhead** compared to a standard model, making it practical for real-world large-scale training.\n",
      "\n",
      "**key_takeaways**:\n",
      "- Multi-stream designs significantly increase Memory I/O (Input/Output) overhead.\n",
      "- Kernel Fusion reduces 'Memory Wall' bottlenecks by keeping data in GPU shared memory.\n",
      "- mHC achieves only 6.7% overhead despite having a 4x wider residual stream.\n",
      "--------------------------------\n",
      "--------------------------------\n",
      "**name**: Memory Management: Selective Recomputing\n",
      "\n",
      "**summary**: Training massive models with wider streams can quickly run out of VRAM. Selective recomputing helps by trading a little bit of math for a lot of saved memory.\n",
      "\n",
      "**full_explanation**:\n",
      "Because mHC introduces wider streams and more learnable parameters, the intermediate **activations** (the data stored between layers) can become too large for GPU memory (VRAM). To solve this, the authors employ **Selective Recomputing**. Instead of storing all the intermediate activations for the backward pass (gradient calculation), they discard the activations of the mHC kernels after the forward pass. During the backward pass, they simply re-execute the fast mHC kernels on-the-fly. They developed an optimization to determine the 'optimal block size' ($L_r^*$) for recomputing, calculated using the square root of the total layers and expansion factor, as shown in Equation 20: $L_r^* \\approx \\sqrt{\\frac{nL}{n+2}}$. This allows them to maintain a much smaller memory footprint without significantly slowing down the training process, a technique that has become standard in high-performance LLM training like the **DeepSeek-V3** pipeline.\n",
      "\n",
      "**key_takeaways**:\n",
      "- Selective recomputing discards intermediate data and recalculates it during the backward pass.\n",
      "- It trades minor computational cost for significant savings in GPU memory (VRAM).\n",
      "- The optimal recomputing block size is synchronized with pipeline stages for maximum efficiency.\n",
      "--------------------------------\n",
      "--------------------------------\n",
      "**name**: DualPipe: Overlapping Communication and Computation\n",
      "\n",
      "**summary**: In large-scale training, GPUs spend a lot of time waiting for data from other GPUs. mHC uses an extended 'DualPipe' schedule to keep the processors busy while the network is talking.\n",
      "\n",
      "**full_explanation**:\n",
      "Large-scale models use **Pipeline Parallelism**, where different layers are hosted on different GPUs. In mHC, the $n$-stream design increases the amount of data that needs to be communicated between these GPUs. To hide this latency, the researchers extended the **DualPipe** schedule (originally from the DeepSeek-V3 technical report). As illustrated in Figure 4, they overlap the 'Communication Stream' (sending/receiving data) with the 'Compute Stream.' Specifically, they execute the residual merging kernels of **MLP (FFN)** layers on a high-priority compute stream while the network handles the communication of the next block. They also avoid 'persistent kernels' for long-running attention tasks, which allows for more flexible scheduling. This ensures that the GPU never sits idle, even with the increased communication demands of the wider mHC stream.\n",
      "\n",
      "**key_takeaways**:\n",
      "- DualPipe overlaps inter-GPU communication with actual layer computation.\n",
      "- mHC's wider stream increases communication volume, requiring better overlapping strategies.\n",
      "- High-priority compute streams prevent communication bottlenecks from stalling the training.\n",
      "--------------------------------\n",
      "--------------------------------\n",
      "**name**: Results: 27B Model Benchmarks\n",
      "\n",
      "**summary**: Does all this math actually make models smarter? The authors tested mHC on an 27-billion parameter model and saw significant improvements in reasoning and stability.\n",
      "\n",
      "**full_explanation**:\n",
      "The authors validated mHC by training a **27B parameter Mixture-of-Experts (MoE)** model. As shown in Table 4, mHC consistently outperformed both the baseline Transformer and the standard HC across several benchmarks. For instance, on **BBH (Big-Bench Hard)**, mHC achieved 51.0% accuracy compared to 43.8% for the baseline. On the **DROP** reasoning task, it outperformed the baseline by 6.9 points. Perhaps most importantly, the training stability was vastly improved: as shown in Figure 5, mHC effectively mitigated the 'loss surges' seen in unconstrained HC and maintained a stable **gradient norm** profile similar to a standard residual network. This proves that you can have the best of both worlds—the high performance of wide connections and the rock-solid stability of traditional ResNets.\n",
      "\n",
      "**key_takeaways**:\n",
      "- mHC consistently outperforms standard Transformers across diverse benchmarks (BBH, GSM8K, MMLU).\n",
      "- It achieved a 7.2% gain on BBH reasoning over the baseline 27B model.\n",
      "- Stability is verified: loss curves for mHC are smooth, unlike the volatile curves of standard HC.\n",
      "--------------------------------\n",
      "--------------------------------\n",
      "**name**: Modern Context: DeepSeek and the Future of Architecture\n",
      "\n",
      "**summary**: Published by the DeepSeek-AI team, mHC represents a move toward 'Macro-Design'—rethinking how models are structured rather than just making them bigger.\n",
      "\n",
      "**full_explanation**:\n",
      "mHC is part of a broader trend in AI research focusing on **Macro-Architecture**. While most recent breakthroughs (like GPT-4 or Llama-3) use the standard 'decoder-only' Transformer, labs like **DeepSeek** are experimenting with the internal wiring of these models. mHC builds on the same engineering principles used in **DeepSeek-V3**, such as **Multi-Head Latent Attention (MLA)** and **DualPipe**. Since this paper's release (late 2024), the industry has moved toward even more complex 'dynamic architectures' where the flow of information is gated or constrained by manifolds to ensure training never crashes. mHC provides a flexible framework where researchers can swap the 'doubly stochastic' manifold for other geometric constraints, opening the door for next-generation foundational models that are wider, deeper, and more efficient than ever before.\n",
      "\n",
      "**key_takeaways**:\n",
      "- mHC is a key innovation from the DeepSeek-AI team, creators of state-of-the-art open models.\n",
      "- The trend is moving from 'micro-design' (individual layers) to 'macro-design' (layer connections).\n",
      "- Manifold constraints are becoming a standard tool for ensuring the stability of massive neural networks.\n",
      "--------------------------------\n"
     ]
    }
   ],
   "source": [
    "topics = breakdown_obj.topics\n",
    "for topic in topics:\n",
    "    print('--------------------------------')\n",
    "    print(topic.to_text())\n",
    "    print('--------------------------------')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "750e8289",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4a5dcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [03:01<00:00, 18.11s/it]\n"
     ]
    }
   ],
   "source": [
    "# from tqdm import tqdm\n",
    "\n",
    "# i=0\n",
    "\n",
    "# for topic in tqdm(topics):\n",
    "#     storyboard_obj, raw_storyboard_response = eduly_client.storyboard(\n",
    "#         topic=topic,\n",
    "#         source_file=pathlib.Path(\"./deepseek_mhc/deepseek_mhc.pdf\")\n",
    "#     )\n",
    "\n",
    "#     with open(f'./deepseek_mhc/deepseek_mhc_gemini_3_flash_storyboard_{i}.pkl', 'wb') as f:\n",
    "#         pickle.dump(raw_storyboard_response, f)\n",
    "#     i+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e8907d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from eduly.models import TopicStoryboard\n",
    "\n",
    "with open('./deepseek_mhc/deepseek_mhc_gemini_3_flash_storyboard_1.pkl', 'rb') as f:\n",
    "    raw_storyboard_response = pickle.load(f)\n",
    "\n",
    "storyboard_obj = TopicStoryboard.model_validate_json(raw_storyboard_response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db5311ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyper-Connections (HC): Expanding the Residual Stream\n",
      "Visual Concept: The Multi-Lane Super-Highway metaphor. We transform the classic single-lane 'residual highway' of a neural network into a wide, multi-stream thoroughfare where information can change lanes and mix, increasing capacity but risking a massive pile-up of numerical instability.\n",
      "\n",
      "## [hook] The Single-Lane Highway\n",
      "\n",
      "**Visual Description:**\n",
      "On a dark background, a single glowing blue line (the residual stream) flows horizontally from left to right. A block labeled 'Layer F' sits above it. An arrow branches off the line, goes through 'Layer F', and joins back into the line using a plus sign. The equation 'x plus F of x' appears above the plus sign. The line is thin and steady.\n",
      "**Narration:**\n",
      "For nearly a decade, deep learning has relied on a single highway to carry information through layers. These residual connections ensure that even in very deep models, the original signal doesn't get lost. But as our models get bigger, this single lane is starting to feel like a bottleneck.\n",
      "\n",
      "## [mid] Widening the Stream\n",
      "\n",
      "**Visual Description:**\n",
      "The single blue line dramatically expands vertically into a thick bundle of four parallel glowing lines. This is labeled 'Residual Stream (n = 4)'. The input 'x' transforms from a single vector into a small matrix icon. The 'Layer F' block now has a funnel-like 'Pre-Mapping' icon below it and a 'Post-Mapping' icon above it, showing how it pulls information from all four lanes and pushes it back out.\n",
      "**Narration:**\n",
      "Hyper-Connections, or HC, propose a radical change: widening the highway. Instead of one vector, we use an expansion rate—say, four—to create multiple parallel streams. This significantly boosts the model's topological complexity without actually increasing the heavy math within the layers themselves.\n",
      "\n",
      "## [mid] The Mixing Matrices\n",
      "\n",
      "**Visual Description:**\n",
      "Between two layers on the wide highway, a matrix labeled 'H-res' appears. It looks like a grid of semi-transparent squares. As the four parallel lines pass through it, they twist and blend—threads from the top lane move to the bottom, and vice versa. The equation 'x at L plus 1 equals H-res times x' appears at the bottom of the screen.\n",
      "**Narration:**\n",
      "The secret sauce is this: the learnable mixing matrix. It allows information to 'change lanes,' blending features across the wider stream as they travel. This gives the model far more flexibility in how it routes and processes information across different depths.\n",
      "\n",
      "## [mid] The Numerical Explosion\n",
      "\n",
      "**Visual Description:**\n",
      "We zoom out to see 50 layers of this wide highway. Initially, the glowing lines are a calm blue. As they pass through successive 'H-res' mixing matrices, the lines start to glow blindingly white and vibrate violently in some areas, while fading to pitch black in others. A gauge on the side labeled 'Signal Gain' spikes from 1.0 to a staggering 3,000.0.\n",
      "**Narration:**\n",
      "However, this freedom comes with a dangerous 'identity crisis.' Because these mixing matrices are unconstrained, they can accidentally amplify the signal to astronomical levels or squash it into nothingness. In large models, this causes a 'numerical explosion' that makes training completely unstable.\n",
      "\n",
      "## [closing] The Stability Challenge\n",
      "\n",
      "**Visual Description:**\n",
      "The screen splits. On the left, the stable but narrow single-lane ResNet. On the right, the powerful but chaotic multi-lane HC highway. A question mark appears over the HC side. The chaotic white lines on the right slowly begin to align into a structured, constrained grid as the scene fades.\n",
      "**Narration:**\n",
      "Hyper-Connections give us the wide-stream capacity we need for the next generation of AI, but they break the very stability that made ResNets work. To harness this power, we need a way to keep these lanes in check—restricting them to a specific mathematical manifold that restores balance.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(storyboard_obj.topic_name)\n",
    "print(f\"Visual Concept: {storyboard_obj.visual_concept}\\n\")\n",
    "scenes = storyboard_obj.scenes\n",
    "for scene in scenes:\n",
    "    print(f\"{scene.to_text()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2181354c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
