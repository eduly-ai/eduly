{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2217fec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "import pickle\n",
    "\n",
    "import httpx\n",
    "from dotenv import load_dotenv\n",
    "from google import genai\n",
    "from google.genai import types as gemini_types\n",
    "\n",
    "load_dotenv(dotenv_path=\"../../.env\")\n",
    "\n",
    "aistudio_gemini_api_key = os.environ['GOOGLE_API_KEY']\n",
    "print(aistudio_gemini_api_key[:3] + '...' + aistudio_gemini_api_key[-1:])\n",
    "gemini_client = genai.Client(api_key=aistudio_gemini_api_key)\n",
    "\n",
    "MODEL_NAME = \"gemini-3-flash-preview\"\n",
    "\n",
    "\n",
    "# MODEL_NAME = \"gemini-3-pro-preview\"\n",
    "# vertex_gemini_key = os.environ[\"VERTEX_AI_KEY\"]\n",
    "# headers = {\"Authorization\": f\"Basic {vertex_gemini_key}\"}\n",
    "# base_url = \"https://generative-ai-proxy.rcp.us-east-1.data.test.exp-aws.net/v1/proxy/vertex-ai\"\n",
    "\n",
    "# http_options=gemini_types.HttpOptions(\n",
    "#         headers=headers,\n",
    "#         api_version=\"\",\n",
    "#         base_url=base_url,\n",
    "#         httpx_client=httpx.Client(verify=False)\n",
    "#     )\n",
    "\n",
    "\n",
    "# gemini_client = genai.Client(\n",
    "#     api_key=\"dummy_value_to_by_pass_validation\",\n",
    "#     http_options=http_options\n",
    "# )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d62a28fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from eduly import EdulyClient\n",
    "\n",
    "eduly_client = EdulyClient(gemini_client=gemini_client)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad68a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # takes ~30 seconds\n",
    "# breakdown_obj, raw_breakdown_response = eduly_client.breakdown(\n",
    "#     file_path=pathlib.Path(\"./attention_is_all_you_need.pdf\"),\n",
    "#     model=MODEL_NAME,\n",
    "#     thinking_level=\"high\"\n",
    "# )\n",
    "\n",
    "# with open('./attention_is_all_you_need_gemini_3_flash_breakdown.pkl', 'wb') as f:\n",
    "#     pickle.dump(raw_breakdown_response, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d8a72b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from eduly.models import Breakdown\n",
    "\n",
    "with open('./attention_is_all_you_need_gemini_3_flash_breakdown.pkl', 'rb') as f:\n",
    "    raw_breakdown_response = pickle.load(f)\n",
    "\n",
    "breakdown_obj = Breakdown.model_validate_json(raw_breakdown_response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "78afa449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "**name**: Beyond Recurrence: The Motivation for Transformers\n",
      "\n",
      "**summary**: Before Transformers, sequence modeling relied on Recurrent Neural Networks (RNNs) that processed data one step at a time, creating a massive bottleneck for parallelization. The Transformer was designed to break this sequential constraint, allowing models to process entire sequences simultaneously.\n",
      "\n",
      "**full_explanation**:\n",
      "In 2017, the dominant models for sequence tasks like translation were **Recurrent Neural Networks (RNNs)**, such as LSTMs and GRUs. These models process data sequentially: to calculate the state of the current word, the model must first calculate the state of the previous word ($h_{t-1}$). This inherent sequential nature prevents parallelization during training, which becomes a critical problem as datasets and sequence lengths grow. While factorization tricks and conditional computation provided some speedups, the fundamental constraint of sequential processing remained. The **Transformer** architecture proposed in this paper eliminates recurrence entirely. By using **attention mechanisms**, it allows the model to draw global dependencies between input and output positions in a single pass. This architectural shift allows for significantly more parallelization on modern hardware like GPUs. **Modern Context**: Since 2017, this move away from recurrence has enabled the training of Large Language Models (LLMs) with trillions of parameters. While the original paper focused on translation, the parallel nature of Transformers eventually led to the development of the 'Foundation Model' era, where models like GPT-4 are trained on nearly the entire internet.\n",
      "\n",
      "**key_takeaways**:\n",
      "- RNNs are limited by sequential computation, which hinders parallel training.\n",
      "- The Transformer replaces recurrence with attention to enable global dependency modeling.\n",
      "- Parallel processing allows for much faster training on larger datasets compared to traditional RNNs.\n",
      "--------------------------------\n",
      "--------------------------------\n",
      "**name**: The Core Mechanism: Scaled Dot-Product Attention\n",
      "\n",
      "**summary**: Scaled Dot-Product Attention is the mathematical engine of the Transformer, allowing the model to decide which parts of a sequence are most relevant to a specific word.\n",
      "\n",
      "**full_explanation**:\n",
      "The Transformer uses a specific type of attention called **Scaled Dot-Product Attention**. It operates on three main vectors: **Queries (Q)**, **Keys (K)**, and **Values (V)**. Conceptually, a Query is what you are looking for, a Key is a label for information, and a Value is the actual information. The model calculates the dot product of the Query with all Keys, divides by the square root of the dimension of the keys ($\\sqrt{d_k}$), and applies a **softmax** function to obtain weights. These weights are then used to create a weighted sum of the Values. The formula is expressed as: $Attention(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{d_k}})V$. The scaling factor $\\frac{1}{\\sqrt{d_k}}$ is crucial; without it, for large dimensions, the dot products could grow very large, pushing the softmax function into regions with extremely small gradients, which stalls learning. **Modern Context**: While this formula remains the standard, modern implementations often use **FlashAttention**, an IO-aware algorithm that optimizes how these matrices are moved through memory to speed up computation by 2-4x without changing the underlying math.\n",
      "\n",
      "**key_takeaways**:\n",
      "- Attention uses Query, Key, and Value vectors to determine relevance.\n",
      "- The scaling factor (square root of the dimension) prevents vanishing gradients during training.\n",
      "- Softmax creates a probability distribution that 'weights' the most important information.\n",
      "--------------------------------\n",
      "--------------------------------\n",
      "**name**: Multi-Head Attention: Viewing Information from Different Perspectives\n",
      "\n",
      "**summary**: Instead of performing a single attention function, the Transformer uses 'Multi-Head Attention' to look at different parts of the input sequence simultaneously through different representation subspaces.\n",
      "\n",
      "**full_explanation**:\n",
      "Rather than calculating attention once with large vectors, the paper found it beneficial to linearly project the Queries, Keys, and Values multiple times ($h$ times) into smaller dimensions. These are called **attention heads**. Each head performs the attention function in parallel. The results of all heads are concatenated and projected again to produce a final output. This allows the model to jointly attend to information from different representation subspaces at different positions. For example, in a sentence, one head might focus on grammatical relationships (like which noun goes with which verb), while another head might focus on semantic meaning or coreference resolution (identifying that 'he' refers to 'John'). Figure 2 in the paper illustrates this by showing multiple 'Scaled Dot-Product Attention' blocks feeding into a 'Concat' and then a 'Linear' layer. **Modern Context**: Most modern Transformers use 8 to 16 heads. Some newer architectures use **Grouped-Query Attention (GQA)**, which shares keys and values across several queries to reduce memory usage and speed up inference, a technique used in models like Llama 3.\n",
      "\n",
      "**key_takeaways**:\n",
      "- Multi-Head Attention allows the model to attend to different types of relationships simultaneously.\n",
      "- It splits the model's 'focus' into several parallel streams called heads.\n",
      "- Concatenating these heads provides a richer, more nuanced representation of the sequence.\n",
      "--------------------------------\n",
      "--------------------------------\n",
      "**name**: The Encoder-Decoder Stack\n",
      "\n",
      "**summary**: The original Transformer consists of two main parts: an Encoder that reads the input and a Decoder that generates the output, both built from stacks of identical layers.\n",
      "\n",
      "**full_explanation**:\n",
      "The Transformer follows an **Encoder-Decoder** structure, as shown in Figure 1. The **Encoder** (on the left) is composed of a stack of $N=6$ identical layers. Each layer has two sub-layers: a multi-head self-attention mechanism and a position-wise fully connected feed-forward network. Around each sub-layer, there is a **residual connection** followed by **Layer Normalization**, expressed as $LayerNorm(x + Sublayer(x))$. The **Decoder** (on the right) is also a stack of $N=6$ identical layers. In addition to the two sub-layers found in the encoder, the decoder includes a third sub-layer that performs multi-head attention over the encoder's output. Crucially, the decoder's self-attention is **masked** to prevent it from 'looking ahead' at future words during training, ensuring it only uses previously generated words to predict the next. **Modern Context**: While the original paper used this dual structure for translation, the field has since diverged. Models like **BERT** are Encoder-only (good for understanding text), while models like **GPT** are Decoder-only (good for generating text). Most modern Generative AI follows the Decoder-only path.\n",
      "\n",
      "**key_takeaways**:\n",
      "- The Encoder processes the input sequence; the Decoder generates the output sequence.\n",
      "- Residual connections and Layer Normalization are used to help deep networks train effectively.\n",
      "- The Decoder is masked to ensure the auto-regressive propertyâ€”predicting the next word based only on previous ones.\n",
      "--------------------------------\n",
      "--------------------------------\n",
      "**name**: Positional Encoding: Adding Order to the Chaos\n",
      "\n",
      "**summary**: Since Transformers process all words at once, they have no inherent sense of word order. Positional Encodings are added to the input to tell the model where each word sits in a sentence.\n",
      "\n",
      "**full_explanation**:\n",
      "Because the Transformer contains no recurrence or convolution, it sees an input sequence as a 'bag of words' with no inherent order. To give the model information about the relative or absolute position of tokens, the authors inject **Positional Encodings** into the input embeddings. In this paper, they used sine and cosine functions of different frequencies: $PE_{(pos, 2i)} = sin(pos/10000^{2i/d_{model}})$ and $PE_{(pos, 2i+1)} = cos(pos/10000^{2i/d_{model}})$. These values are added (not concatenated) to the word embeddings. This specific sinusoidal pattern was chosen because the authors hypothesized it would allow the model to easily learn to attend by relative positions, as any fixed offset $k$ can be represented as a linear function of the position. **Modern Context**: While sinusoidal encodings were groundbreaking, many modern models have moved to **learned positional embeddings** or more advanced methods like **Rotary Positional Embeddings (RoPE)** or **ALiBi**, which better handle sequences longer than those seen during training and improve the model's sense of 'distance' between words.\n",
      "\n",
      "**key_takeaways**:\n",
      "- Transformers process tokens in parallel and need a way to represent sequence order.\n",
      "- Positional Encodings use mathematical functions to 'tag' each word with its location.\n",
      "- Sinusoidal encodings allow the model to generalize to different sequence lengths.\n",
      "--------------------------------\n",
      "--------------------------------\n",
      "**name**: Point-wise Feed-Forward Networks\n",
      "\n",
      "**summary**: Every layer in the Transformer contains a simple feed-forward network that processes each word's representation independently.\n",
      "\n",
      "**full_explanation**:\n",
      "In addition to attention sub-layers, each encoder and decoder layer contains a **Position-wise Feed-Forward Network (FFN)**. This network is applied to each position separately and identically. It consists of two linear transformations with a **ReLU activation** in between: $FFN(x) = max(0, xW_1 + b_1)W_2 + b_2$. The dimensionality of the input and output is $d_{model} = 512$, while the inner layer is larger, with a dimensionality of $d_{ff} = 2048$. Think of this as a 'processing' step where each word, having gathered information from other words via attention, now updates its own internal representation. **Modern Context**: Many modern Transformers have replaced the ReLU activation with more efficient variants like **GeLU (Gaussian Error Linear Unit)** or **SwiGLU**. Additionally, in very large models, these feed-forward blocks often account for the majority of the model's total parameters.\n",
      "\n",
      "**key_takeaways**:\n",
      "- The Feed-Forward Network processes each position in the sequence independently.\n",
      "- It consists of two linear layers and a non-linear activation function.\n",
      "- This component allows the model to perform deeper computation on the features gathered by the attention heads.\n",
      "--------------------------------\n",
      "--------------------------------\n",
      "**name**: Computational Efficiency and Scaling\n",
      "\n",
      "**summary**: The Transformer's design allows it to handle long-range dependencies more efficiently than RNNs or CNNs, leading to state-of-the-art results at a lower training cost.\n",
      "\n",
      "**full_explanation**:\n",
      "The paper compares the 'Self-Attention' layer to 'Recurrent' and 'Convolutional' layers across three criteria: total computational complexity, amount of parallelizable computation, and path length between long-range dependencies. In terms of path length, self-attention connects all positions with a constant $O(1)$ number of operations, whereas RNNs require $O(n)$ operations to connect the first word to the last. This makes it much easier for Transformers to learn relationships between distant words. Table 1 in the paper highlights that for sequence lengths ($n$) smaller than the representation dimension ($d$), which is typical, self-attention is also faster. In 2017, the Transformer (big) model achieved a **BLEU score of 28.4** on English-to-German and **41.8** on English-to-French, outperforming previous state-of-the-art ensembles while training in a fraction of the time. **Modern Context**: While $O(n^2)$ complexity was an improvement over RNNs for medium lengths, it becomes a massive bottleneck for very long sequences (e.g., entire books). This has led to the development of 'Linear Transformers' and sparse attention mechanisms to handle the massive 'context windows' seen in modern models like Claude 3 or Gemini.\n",
      "\n",
      "**key_takeaways**:\n",
      "- Self-attention has a constant path length for long-range dependencies, aiding learning.\n",
      "- Transformers are significantly faster to train than RNNs because they are highly parallelizable.\n",
      "- The original Transformer set new accuracy records (BLEU scores) while being more efficient.\n",
      "--------------------------------\n",
      "--------------------------------\n",
      "**name**: Self-Attention Visualized: How the Model 'Thinks'\n",
      "\n",
      "**summary**: Visualizing attention distributions reveals that the model learns to understand the syntactic and semantic structure of sentences without being explicitly told how.\n",
      "\n",
      "**full_explanation**:\n",
      "The authors provided visualizations of how different attention heads behave in Figure 3, 4, and 5. For example, in Figure 3, we see the model processing the word 'making' in a complex sentence. Various attention heads attend to the distant objects and verbs that complete the phrase ('making... more difficult'), effectively resolving long-distance dependencies. Figure 4 shows heads performing **anaphora resolution**, where the word 'its' attends strongly to the noun it refers to ('Law'). These visualizations prove that the model isn't just crunching numbers; it's learning to build a logical structure of the language. Each head can specialize: some focus on the next word, some on the previous word, and some on the overall sentence structure. **Modern Context**: Tools like 'BertViz' now allow researchers to peer into modern LLMs. We've discovered that specific 'neurons' or 'heads' in models like GPT-4 specialize in things like identifying code blocks, checking for grammatical errors, or even tracking the sentiment of a paragraph.\n",
      "\n",
      "**key_takeaways**:\n",
      "- Attention heads learn to perform specific linguistic tasks like resolving pronouns.\n",
      "- Visualizations show the model can link words across long distances to understand context.\n",
      "- The model develops an internal understanding of grammar and logic through pure data exposure.\n",
      "--------------------------------\n"
     ]
    }
   ],
   "source": [
    "topics = breakdown_obj.topics\n",
    "for topic in topics:\n",
    "    print('--------------------------------')\n",
    "    print(topic.to_text())\n",
    "    print('--------------------------------')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "da4a5dcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'get'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m i=\u001b[32m0\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m topic \u001b[38;5;129;01min\u001b[39;00m tqdm(topics):\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     storyboard_obj, raw_storyboard_response = \u001b[43meduly_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstoryboard\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtopic\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtopic\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# source_file=pathlib.Path(\"./attention_is_all_you_need.pdf\")\u001b[39;49;00m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m./attention_is_all_you_need_gemini_3_flash_storyboard_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.pkl\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mwb\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m     12\u001b[39m         pickle.dump(raw_storyboard_response, f)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/eduly-ai/eduly/.venv/lib/python3.12/site-packages/eduly/client.py:91\u001b[39m, in \u001b[36mstoryboard\u001b[39m\u001b[34m(self, topic, source_file, model, thinking_level)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/eduly-ai/eduly/.venv/lib/python3.12/site-packages/google/genai/models.py:5203\u001b[39m, in \u001b[36mModels.generate_content\u001b[39m\u001b[34m(self, model, contents, config)\u001b[39m\n\u001b[32m   5201\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m remaining_remote_calls_afc > \u001b[32m0\u001b[39m:\n\u001b[32m   5202\u001b[39m   i += \u001b[32m1\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m5203\u001b[39m   response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   5204\u001b[39m \u001b[43m      \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontents\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparsed_config\u001b[49m\n\u001b[32m   5205\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   5207\u001b[39m   function_map = _extra_utils.get_function_map(parsed_config)\n\u001b[32m   5208\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m function_map:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/eduly-ai/eduly/.venv/lib/python3.12/site-packages/google/genai/models.py:3985\u001b[39m, in \u001b[36mModels._generate_content\u001b[39m\u001b[34m(self, model, contents, config)\u001b[39m\n\u001b[32m   3982\u001b[39m request_dict = _common.convert_to_dict(request_dict)\n\u001b[32m   3983\u001b[39m request_dict = _common.encode_unserializable_types(request_dict)\n\u001b[32m-> \u001b[39m\u001b[32m3985\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_api_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3986\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mpost\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhttp_options\u001b[49m\n\u001b[32m   3987\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3989\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\n\u001b[32m   3990\u001b[39m     config, \u001b[33m'\u001b[39m\u001b[33mshould_return_http_response\u001b[39m\u001b[33m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   3991\u001b[39m ):\n\u001b[32m   3992\u001b[39m   return_value = types.GenerateContentResponse(sdk_http_response=response)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/eduly-ai/eduly/.venv/lib/python3.12/site-packages/google/genai/_api_client.py:1388\u001b[39m, in \u001b[36mBaseApiClient.request\u001b[39m\u001b[34m(self, http_method, path, request_dict, http_options)\u001b[39m\n\u001b[32m   1378\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrequest\u001b[39m(\n\u001b[32m   1379\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1380\u001b[39m     http_method: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1383\u001b[39m     http_options: Optional[HttpOptionsOrDict] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1384\u001b[39m ) -> SdkHttpResponse:\n\u001b[32m   1385\u001b[39m   http_request = \u001b[38;5;28mself\u001b[39m._build_request(\n\u001b[32m   1386\u001b[39m       http_method, path, request_dict, http_options\n\u001b[32m   1387\u001b[39m   )\n\u001b[32m-> \u001b[39m\u001b[32m1388\u001b[39m   response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhttp_request\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhttp_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   1389\u001b[39m   response_body = (\n\u001b[32m   1390\u001b[39m       response.response_stream[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m response.response_stream \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m   1391\u001b[39m   )\n\u001b[32m   1392\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m SdkHttpResponse(headers=response.headers, body=response_body)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/eduly-ai/eduly/.venv/lib/python3.12/site-packages/google/genai/_api_client.py:1224\u001b[39m, in \u001b[36mBaseApiClient._request\u001b[39m\u001b[34m(self, http_request, http_options, stream)\u001b[39m\n\u001b[32m   1221\u001b[39m     retry = tenacity.Retrying(**retry_kwargs)\n\u001b[32m   1222\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m retry(\u001b[38;5;28mself\u001b[39m._request_once, http_request, stream)  \u001b[38;5;66;03m# type: ignore[no-any-return]\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1224\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_retry\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request_once\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhttp_request\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/eduly-ai/eduly/.venv/lib/python3.12/site-packages/tenacity/__init__.py:477\u001b[39m, in \u001b[36mRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    475\u001b[39m retry_state = RetryCallState(retry_object=\u001b[38;5;28mself\u001b[39m, fn=fn, args=args, kwargs=kwargs)\n\u001b[32m    476\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m477\u001b[39m     do = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    478\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[32m    479\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/eduly-ai/eduly/.venv/lib/python3.12/site-packages/tenacity/__init__.py:378\u001b[39m, in \u001b[36mBaseRetrying.iter\u001b[39m\u001b[34m(self, retry_state)\u001b[39m\n\u001b[32m    376\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    377\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter_state.actions:\n\u001b[32m--> \u001b[39m\u001b[32m378\u001b[39m     result = \u001b[43maction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    379\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/eduly-ai/eduly/.venv/lib/python3.12/site-packages/tenacity/__init__.py:420\u001b[39m, in \u001b[36mBaseRetrying._post_stop_check_actions.<locals>.exc_check\u001b[39m\u001b[34m(rs)\u001b[39m\n\u001b[32m    418\u001b[39m retry_exc = \u001b[38;5;28mself\u001b[39m.retry_error_cls(fut)\n\u001b[32m    419\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.reraise:\n\u001b[32m--> \u001b[39m\u001b[32m420\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43mretry_exc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    421\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m retry_exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mfut\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexception\u001b[39;00m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/eduly-ai/eduly/.venv/lib/python3.12/site-packages/tenacity/__init__.py:187\u001b[39m, in \u001b[36mRetryError.reraise\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    185\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mreraise\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> t.NoReturn:\n\u001b[32m    186\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.last_attempt.failed:\n\u001b[32m--> \u001b[39m\u001b[32m187\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlast_attempt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    188\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.12.12-macos-aarch64-none/lib/python3.12/concurrent/futures/_base.py:449\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    447\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[38;5;28mself\u001b[39m._condition.wait(timeout)\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.12.12-macos-aarch64-none/lib/python3.12/concurrent/futures/_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception:\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/eduly-ai/eduly/.venv/lib/python3.12/site-packages/tenacity/__init__.py:480\u001b[39m, in \u001b[36mRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    478\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[32m    479\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m480\u001b[39m         result = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    481\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n\u001b[32m    482\u001b[39m         retry_state.set_exception(sys.exc_info())  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/eduly-ai/eduly/.venv/lib/python3.12/site-packages/google/genai/_api_client.py:1201\u001b[39m, in \u001b[36mBaseApiClient._request_once\u001b[39m\u001b[34m(self, http_request, stream)\u001b[39m\n\u001b[32m   1193\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1194\u001b[39m   response = \u001b[38;5;28mself\u001b[39m._httpx_client.request(\n\u001b[32m   1195\u001b[39m       method=http_request.method,\n\u001b[32m   1196\u001b[39m       url=http_request.url,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1199\u001b[39m       timeout=http_request.timeout,\n\u001b[32m   1200\u001b[39m   )\n\u001b[32m-> \u001b[39m\u001b[32m1201\u001b[39m   \u001b[43merrors\u001b[49m\u001b[43m.\u001b[49m\u001b[43mAPIError\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1202\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m HttpResponse(\n\u001b[32m   1203\u001b[39m       response.headers, response \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m [response.text]\n\u001b[32m   1204\u001b[39m   )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/eduly-ai/eduly/.venv/lib/python3.12/site-packages/google/genai/errors.py:121\u001b[39m, in \u001b[36mAPIError.raise_for_response\u001b[39m\u001b[34m(cls, response)\u001b[39m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    119\u001b[39m   response_json = response.body_segments[\u001b[32m0\u001b[39m].get(\u001b[33m'\u001b[39m\u001b[33merror\u001b[39m\u001b[33m'\u001b[39m, {})\n\u001b[32m--> \u001b[39m\u001b[32m121\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mraise_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstatus_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_json\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/eduly-ai/eduly/.venv/lib/python3.12/site-packages/google/genai/errors.py:146\u001b[39m, in \u001b[36mAPIError.raise_error\u001b[39m\u001b[34m(cls, status_code, response_json, response)\u001b[39m\n\u001b[32m    132\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Raises an appropriate APIError subclass based on the status code.\u001b[39;00m\n\u001b[32m    133\u001b[39m \n\u001b[32m    134\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    143\u001b[39m \u001b[33;03m  APIError: For other error status codes.\u001b[39;00m\n\u001b[32m    144\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[32m400\u001b[39m <= status_code < \u001b[32m500\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m146\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43mClientError\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstatus_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_json\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    147\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[32m500\u001b[39m <= status_code < \u001b[32m600\u001b[39m:\n\u001b[32m    148\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m ServerError(status_code, response_json, response)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/eduly-ai/eduly/.venv/lib/python3.12/site-packages/google/genai/errors.py:50\u001b[39m, in \u001b[36mAPIError.__init__\u001b[39m\u001b[34m(self, code, response_json, response)\u001b[39m\n\u001b[32m     48\u001b[39m \u001b[38;5;28mself\u001b[39m.response = response\n\u001b[32m     49\u001b[39m \u001b[38;5;28mself\u001b[39m.details = response_json\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m \u001b[38;5;28mself\u001b[39m.message = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse_json\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     51\u001b[39m \u001b[38;5;28mself\u001b[39m.status = \u001b[38;5;28mself\u001b[39m._get_status(response_json)\n\u001b[32m     52\u001b[39m \u001b[38;5;28mself\u001b[39m.code = code \u001b[38;5;28;01mif\u001b[39;00m code \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m._get_code(response_json)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/eduly-ai/eduly/.venv/lib/python3.12/site-packages/google/genai/errors.py:78\u001b[39m, in \u001b[36mAPIError._get_message\u001b[39m\u001b[34m(self, response_json)\u001b[39m\n\u001b[32m     76\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_message\u001b[39m(\u001b[38;5;28mself\u001b[39m, response_json: Any) -> Any:\n\u001b[32m     77\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m response_json.get(\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m       \u001b[33m'\u001b[39m\u001b[33mmessage\u001b[39m\u001b[33m'\u001b[39m, \u001b[43mresponse_json\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43merror\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m(\u001b[33m'\u001b[39m\u001b[33mmessage\u001b[39m\u001b[33m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m     79\u001b[39m   )\n",
      "\u001b[31mAttributeError\u001b[39m: 'str' object has no attribute 'get'"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "i=0\n",
    "\n",
    "for topic in tqdm(topics):\n",
    "    storyboard_obj, raw_storyboard_response = eduly_client.storyboard(\n",
    "        topic=topic,\n",
    "        # source_file=pathlib.Path(\"./attention_is_all_you_need.pdf\")\n",
    "    )\n",
    "\n",
    "    with open(f'./attention_is_all_you_need_gemini_3_flash_storyboard_{i}.pkl', 'wb') as f:\n",
    "        pickle.dump(raw_storyboard_response, f)\n",
    "    i+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3e8907d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from eduly.models import TopicStoryboard\n",
    "\n",
    "with open('./attention_is_all_you_need_gemini_3_flash_storyboard_1.pkl', 'rb') as f:\n",
    "    raw_storyboard_response = pickle.load(f)\n",
    "\n",
    "storyboard_obj = TopicStoryboard.model_validate_json(raw_storyboard_response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "db5311ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Core Mechanism: Scaled Dot-Product Attention\n",
      "Visual Concept: The search-and-retrieval metaphor: A blue 'Query' vector acts as a spotlight, scanning across golden 'Key' vectors to determine which green 'Value' vectors should be retrieved and blended into the final output.\n",
      "\n",
      "## [hook] The Library of Vectors\n",
      "On a dark background, three distinct clusters of colorful boxes appear. The first cluster, labeled 'Queries' (Q), is a set of deep blue 1x4 horizontal rectangles. The second, labeled 'Keys' (K), is a set of golden yellow 1x4 horizontal rectangles. The third, 'Values' (V), consists of emerald green 1x4 horizontal rectangles. A single blue Query vector (q1) floats to the center. Below it, three golden Key vectors (k1, k2, k3) are lined up horizontally. The Query vector pulses, and faint gray arrows point from it to each Key, suggesting a search process. Text appears: 'What are we looking for? (Query) vs. What information is available? (Key)'.\n",
      "\n",
      "## [mid] Measuring Similarity: The Dot Product\n",
      "The blue Query (q1) and the first golden Key (k1) align vertically. They expand slightly to show their numerical components: q1 = [0.5, 0.1, 0.8, -0.2] and k1 = [0.4, 0.0, 0.9, 0.1]. As they overlap, the corresponding elements multiply and sum: (0.5*0.4) + (0.1*0.0) + (0.8*0.9) + (-0.2*0.1) = 0.9. This result, 0.9, appears as a white scalar value above them labeled 'Score'. This repeats for k2 and k3, resulting in scores of 0.1 and -0.4. The scores are arranged in a row matrix labeled 'QK^T'. The visual emphasizes that higher overlap creates a higher score.\n",
      "\n",
      "## [mid] The Scaling Factor: Safety Valve\n",
      "The scores [0.9, 0.1, -0.4] are suddenly replaced by much larger numbers, like [90, 10, -40], to simulate a high-dimensional space where d_k = 64. A graph of the Softmax function (a steep S-curve) appears on the right. Large values like 90 are shown far into the 'flat' region of the curve where the slope (gradient) is nearly zero. A warning symbol flashes: 'Vanishing Gradients!'. The formula component '1 / sqrt(d_k)' appears. Since d_k = 64, sqrt(d_k) = 8. The large scores are divided by 8, shrinking them back to [11.25, 1.25, -5.0]. The points on the Softmax curve slide back toward the center where the slope is steeper, indicating healthy learning.\n",
      "\n",
      "## [mid] Softmax and the Weighted Sum\n",
      "The scaled scores enter a 'Softmax' box. Out come three probabilities (weights) that sum to 1.0: [0.85, 0.14, 0.01]. These weights are represented as semi-transparent orange overlays. These orange overlays move down to the emerald green Value vectors (v1, v2, v3). v1 becomes bright and opaque (85% weight), v2 becomes dim (14%), and v3 almost disappears (1%). The three Value vectors then physically merge/collapse into a single new vector: the 'Attention Output'. This output is a weighted average, dominated by the information in v1.\n",
      "\n",
      "## [closing] The Full Engine\n",
      "The entire process zooms out to show the complete formula at the top: Attention(Q, K, V) = softmax(QK^T / sqrt(d_k))V. The matrices Q, K, and V are shown as large grids. A highlight passes over the matrices to show 'FlashAttention' - instead of calculating the whole huge matrix at once, the screen divides the matrices into smaller 'tiles' or blocks that move rapidly between memory and the processor. The blocks glow as they are processed in quick succession, illustrating a 2-4x speedup. The final Attention Output vector glows brightly, symbolizing the perfectly distilled information ready for the next layer.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(storyboard_obj.topic_name)\n",
    "print(f\"Visual Concept: {storyboard_obj.visual_concept}\\n\")\n",
    "scenes = storyboard_obj.scenes\n",
    "for scene in scenes:\n",
    "    print(f\"{scene.to_text()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2181354c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
