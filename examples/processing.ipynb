{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c75f68e2",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c92efe9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AIz...8\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pathlib\n",
    "import pickle\n",
    "\n",
    "import httpx\n",
    "from dotenv import load_dotenv\n",
    "from google import genai\n",
    "from google.genai import types as gemini_types\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from tqdm import tqdm\n",
    "\n",
    "from eduly import EdulyAnimationClient, EdulyBreakdownClient\n",
    "\n",
    "load_dotenv(dotenv_path=\"../.env\")\n",
    "\n",
    "aistudio_gemini_api_key = os.environ['GOOGLE_API_KEY']\n",
    "print(aistudio_gemini_api_key[:3] + '...' + aistudio_gemini_api_key[-1:])\n",
    "gemini_client = genai.Client(api_key=aistudio_gemini_api_key)\n",
    "\n",
    "MODEL_NAME = \"gemini-3-flash-preview\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0b21b1",
   "metadata": {},
   "source": [
    "# Breakdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4e6151d",
   "metadata": {},
   "outputs": [],
   "source": [
    "eduly_breakdown_client = EdulyBreakdownClient(gemini_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25699f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# breakdown_obj, raw_breakdown_response = eduly_breakdown_client.breakdown(\n",
    "#     file_path=pathlib.Path(\"./attention_is_all_you_need/attention_is_all_you_need.pdf\"),\n",
    "#     model=MODEL_NAME,\n",
    "#     thinking_level=\"high\"\n",
    "# )\n",
    "\n",
    "# with open('./attention_is_all_you_need/cached_outputs/attention_is_all_you_need_breakdown.pkl', 'wb') as f:\n",
    "#     pickle.dump(breakdown_obj, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f75a1ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./attention_is_all_you_need/cached_outputs/attention_is_all_you_need_breakdown.pkl', 'rb') as f:\n",
    "    breakdown_obj = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df745627",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'attention_is_all_you_need'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "santised_title = breakdown_obj.document_title.replace(\" \", \"_\").lower()\n",
    "santised_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3160501a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: The Shift from Recurrence to Attention\n",
      "Topic 1: The Encoder-Decoder Structure\n",
      "Topic 2: Scaled Dot-Product Attention: The Mathematical Core\n",
      "Topic 3: Multi-Head Attention: Seeing in Parallel\n",
      "Topic 4: Positional Encoding: Giving Order to Chaos\n",
      "Topic 5: Efficiency and Complexity: Why Transformers Scaled\n",
      "Topic 6: Results and Historical Significance\n"
     ]
    }
   ],
   "source": [
    "for i, topic in enumerate(breakdown_obj.topics):\n",
    "    print(f\"Topic {i}: {topic.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1db9b47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# storyboards = {}\n",
    "\n",
    "# for i, topic in tqdm(enumerate(breakdown_obj.topics)):\n",
    "#     storyboard_obj, raw_storyboard_response = eduly_breakdown_client.storyboard(\n",
    "#         topic=topic,\n",
    "#         model=MODEL_NAME,\n",
    "#         thinking_level=\"high\",\n",
    "#         # source_file=\"/./attention_is_all_you_need/attention_is_all_you_need.pdf\"\n",
    "#     )\n",
    "\n",
    "#     storyboards[topic.name] = storyboard_obj\n",
    "\n",
    "#     with open(f'./cached_outputs/{santised_title}_storyboard_{i}.pkl', 'wb') as f:\n",
    "#         pickle.dump(storyboard_obj, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46d5381d",
   "metadata": {},
   "outputs": [],
   "source": [
    "storyboards = {}\n",
    "\n",
    "for i, topic in enumerate(breakdown_obj.topics):\n",
    "    with open(f'./attention_is_all_you_need/cached_outputs/{santised_title}_storyboard_{i}.pkl', 'rb') as f:\n",
    "        storyboards[topic.name] = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a0d5e39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'The Shift from Recurrence to Attention': TopicStoryboard(topic_name='The Shift from Recurrence to Attention', visual_concept=\"The central metaphor is a 'Conveyor Belt vs. a Global Spotlight.' We visualize Recurrent Neural Networks (RNNs) as a linear, one-at-a-time assembly line that loses detail over distance, contrasted with Attention as a lightning-fast web where every part of a sentence connects to every other part simultaneously.\", scenes=[Scene(scene_type='hook', title='The Sequential Bottleneck', visual_description=\"A long sentence: 'The curious cat, which had spent all morning chasing a bright red laser pointer across the living room floor, finally fell asleep.' The words enter a narrow tube one by one. As each word enters, a small glowing box (the 'hidden state') moves to the next word. It looks slow and rhythmic, like a pulse. As the pulse reaches the end of the sentence, the glow from the word 'cat' at the beginning has almost completely faded away.\", narration='For years, teaching computers to understand language felt like watching a slow-moving assembly line. To process a sentence, AI models had to read one word at a time, carrying a tiny suitcase of memory from start to finish. But by the time they reached the end, that memory often became a blurry mess.'), Scene(scene_type='mid', title='The Mechanics of Recurrence', visual_description=\"We zoom into three words: 'The', 'curious', 'cat'. Below them, a math notation appears: h-sub-t = f(h-sub-t-minus-one, x-sub-t). An arrow goes from the 'The' box to the 'curious' box, carrying a vector of numbers. Then an arrow goes from 'curious' to 'cat'. The boxes are colored blue. The 'cat' box only knows about 'curious' because it was handed a message, not because it can see the word directly.\", narration=\"This is Recurrence. In models like LSTMs, each step depends entirely on the one before it. It’s a chain reaction. This creates two massive problems: it’s impossible to skip ahead, and the 'memory' of the beginning gets diluted as the chain grows longer.\"), Scene(scene_type='mid', title='The Training Wall', visual_description=\"The screen splits. On the left, a single GPU icon processes the 'conveyor belt' sentence slowly. A progress bar crawls. On the right, we see a vast empty space. Text appears: 'Idle Resources'. The sequential nature means we can't use more power to go faster, because step 100 must wait for step 99.\", narration=\"Because of this step-by-step nature, we couldn't just throw more hardware at the problem. Training was stuck in the slow lane, restricted by the speed of the sequence itself, no matter how many GPUs we had.\"), Scene(scene_type='mid', title='Attention is All You Need', visual_description=\"The conveyor belt tube shatters. The sentence 'The cat ... fell asleep' is now laid out in a circle. Suddenly, lines of light flash simultaneously between every single word. The word 'asleep' lights up, and bright gold lines connect it directly back to 'cat' and 'curious', bypassing everything in between.\", narration='Then, in 2017, a landmark paper proposed a radical shift: Attention. What if we stopped the chain and just let every word look at every other word at the exact same time? No more waiting, and no more distance.'), Scene(scene_type='mid', title='The Power of the Matrix', visual_description=\"The circular connections transform into a 15x15 grid (an Attention Map). The rows and columns are labeled with the words of our sentence. Cells where words relate (like 'cat' and 'asleep') glow intensely bright. The entire grid populates with colors all at once, rather than one cell at a time.\", narration='In an Attention-based model, like the Transformer, we calculate all these relationships using matrix multiplication. This is a game-changer because GPUs are specifically designed to do these massive math blocks in parallel, instantly.'), Scene(scene_type='mid', title='The Scaling Revolution', visual_description=\"We zoom out to see thousands of these grids stacking on top of each other. A graph appears showing 'Model Size' on the X-axis and 'Capability' on the Y-axis. The line for RNNs plateaus early, but the line for Transformers shoots upward exponentially. Icons for GPT-1, GPT-2, GPT-3, and GPT-4 appear along the rising curve.\", narration=\"Removing the sequential bottleneck allowed us to scale. We could train on the entire internet and build models with billions of parameters. This shift is exactly what sparked the modern AI explosion we're seeing today.\"), Scene(scene_type='mid', title='The Trade-off: Context Cost', visual_description=\"The 15x15 grid grows to 1000x1000, then 10,000x10,000. As it grows, it starts to glow red and then 'crack' under its own weight. A label appears: 'Computational Cost = Length Squared'. The screen shows that as sequences get very long, the memory needed to store every connection becomes overwhelming.\", narration=\"But there's a catch. While Attention is fast, it's also hungry. Since every word looks at every other word, doubling the sentence length quadruples the work. This 'quadratic cost' makes processing entire books very difficult for standard Transformers.\"), Scene(scene_type='closing', title='Full Circle: The New Recurrence', visual_description=\"A new architecture appears, labeled 'Mamba / SSM'. It looks like a hybrid: it has the sleek, fast flow of the conveyor belt but the high-tech glow of Attention. It moves linearly like an RNN but doesn't lose the color/memory of the first word. The 'cat' stays bright gold all the way to the end.\", narration=\"Interestingly, the story isn't over. New models, like Mamba, are trying to bring back the efficiency of recurrence while keeping the long-term memory of attention. We're finding that the best way forward might be a smarter way to remember the past.\")]),\n",
       " 'The Encoder-Decoder Structure': TopicStoryboard(topic_name='The Encoder-Decoder Structure', visual_concept=\"A 'Two-Tower' architectural metaphor where the Encoder tower acts as an 'Understanding Engine' that converts raw text into a glowing field of context, and the Decoder tower acts as a 'Generation Engine' that reaches into that field to weave a new sequence word-by-word.\", scenes=[Scene(scene_type='hook', title='The Bridge of Translation', visual_description=\"On a dark blue background, the English sentence 'The cat sat on the mat' appears in a clean white font on the left. On the right, the German translation 'Die Katze saß auf der Matte' appears. Between them, two vertical grey rectangles (the 'Towers') emerge. A bridge of golden light pulses between them, connecting the input to the output. The camera pans out to show the symmetry of the two towers.\", narration='How does a machine take a thought in one language and perfectly reconstruct it in another? The secret lies in a dual architecture known as the Encoder and the Decoder. It is a two-tower system designed to first understand, and then create.'), Scene(scene_type='mid', title=\"The Encoder's Layers\", visual_description=\"The left 'Encoder' tower zooms in. We see it's actually a stack of 6 identical blocks labeled 'Layer 1' through 'Layer 6'. The word 'The' enters the bottom of Layer 1 as a simple vector (a small row of 8 colored squares). As it passes through each layer, the vector pulses and changes colors, becoming more complex. By the time it reaches Layer 6, the vector is surrounded by a faint glowing aura.\", narration='The Encoder is the understanding engine. It is made of six identical layers stacked on top of each other. As an input sentence passes through this stack, the model processes the raw words into a sequence of deep, mathematical representations that capture the meaning and context of every word.'), Scene(scene_type='mid', title='Inside the Encoder Layer', visual_description=\"We zoom into a single Encoder block. Inside, we see two distinct stages. The first is a box labeled 'Self-Attention' and the second is 'Feed-Forward'. An arrow shows the data flowing into Self-Attention, then into Feed-Forward. A 'Residual Connection'—a curved arrow—loops around each stage, bypassing the box and merging back into the main line.\", narration=\"Each of these six layers has two main jobs. First, self-attention allows the words to 'talk' to each other to figure out their relationships. Then, a feed-forward network refines that information. These layers are held together by residual connections, which act like express lanes to keep the gradient flowing smoothly during learning.\"), Scene(scene_type='mid', title='The Contextual Cloud', visual_description=\"The Encoder tower finishes processing. The top of the tower now holds a set of six glowing vectors, representing the fully 'understood' sentence. These vectors float in a space between the two towers, creating a 'Contextual Cloud'. Small lines of light shimmer within the cloud, indicating the rich relationships between the words 'cat', 'sat', and 'mat'.\", narration=\"By the time the information leaves the Encoder, it is no longer just a list of words. It is a rich, continuous representation—a sort of 'mental map' of the entire input sentence that the Decoder can now use.\"), Scene(scene_type='mid', title=\"The Decoder's First Move\", visual_description=\"The right 'Decoder' tower activates. It is also a stack of 6 layers. At the bottom, a special 'Start' token enters. Unlike the Encoder which sees everything at once, the Decoder is mostly empty. A single word, 'Die', is generated at the top of the tower and then fed back into the bottom for the next step.\", narration='The Decoder is the generation engine. It works one step at a time, in what we call an auto-regressive process. It starts with a blank slate and generates the first word of the output. But how does it know what to write?'), Scene(scene_type='mid', title='The Encoder-Decoder Attention Bridge', visual_description=\"Inside a Decoder layer, three stages appear: 'Masked Self-Attention', then a new middle stage called 'Encoder-Decoder Attention', then 'Feed-Forward'. A beam of light shoots from the 'Encoder-Decoder Attention' box over to the glowing Contextual Cloud created by the Encoder. It 'grabs' information about the English word 'The' to help generate the German word 'Die'.\", narration=\"This is the magic moment. The Decoder has a third sub-layer called Encoder-Decoder attention. This allows the decoder to 'look back' at the Encoder’s map. While generating each German word, it can focus specifically on the most relevant English words to ensure the translation is accurate.\"), Scene(scene_type='mid', title='Stabilizing the Flow', visual_description=\"A close-up of the boundary between layers. We see a 'Layer Normalization' bar. The vectors passing through are 'squashed' and centered, keeping their values within a specific range (indicated by a small graph showing a bell curve). The residual connection arrows from earlier are highlighted in bright yellow.\", narration='Because these towers are deep, they use Layer Normalization after every step. Think of this as a stabilizer that keeps the mathematical values from exploding or shrinking too much, ensuring the model stays focused and trainable.'), Scene(scene_type='mid', title='The Modern Divergence', visual_description=\"The two towers separate. On the left, the Encoder tower stands alone under the label 'BERT'. On the right, the Decoder tower stands alone under the label 'GPT-4'. In the middle, the original connected two-tower structure is labeled 'T5 / Original Transformer'.\", narration=\"While the original Transformer used both towers for translation, modern AI has branched out. Models like BERT use only the Encoder for understanding tasks, while famous models like GPT-4 are 'Decoder-only,' optimized specifically for generating fluent text one word at a time.\"), Scene(scene_type='closing', title='The Symphony of Sequence', visual_description=\"The full Encoder-Decoder structure is shown again. We see the English sentence flow in, turn into the glowing cloud, and the German sentence emerge word-by-word on the right. The entire system pulses with a rhythmic light. The text 'Efficiency + Context' appears at the bottom.\", narration='The Encoder-Decoder structure changed everything by allowing machines to process sequences with both global context and surgical precision. Whether it is translating languages or writing code, this two-tower dance is the foundation of modern intelligence.')]),\n",
       " 'Scaled Dot-Product Attention: The Mathematical Core': TopicStoryboard(topic_name='Scaled Dot-Product Attention: The Mathematical Core', visual_concept=\"A library search system where 'Search Queries' (Blue) are compared against 'Book Labels' (Red) to extract the most relevant 'Knowledge' (Green). The central visual is the 'Attention Formula' which acts as a lens, focusing light from many sources onto a single point of understanding.\", scenes=[Scene(scene_type='hook', title='The Ambiguity Problem', visual_description=\"On a dark background, the sentence 'The bank of the river' appears in white. The word 'bank' is highlighted with a soft yellow glow. Below it, another sentence appears: 'The bank gave me a loan.' Both words 'bank' are identical. Suddenly, a blue arrow labeled 'Query' emerges from the word 'bank' in the first sentence. It scans the word 'river' which glows red, labeled 'Key'. A connection is made, and the word 'bank' turns a watery blue. In the second sentence, the 'bank' Query scans 'loan' (Key), and the 'bank' turns a money-green. The background fades to a coordinate plane.\", narration=\"To understand a sentence, a computer needs to know which words relate to each other. A word like 'bank' changes meaning based on its neighbors. To solve this, Transformers use a mechanism called Attention to decide which words to focus on. It is essentially a sophisticated search-and-retrieval system.\"), Scene(scene_type='mid', title='Defining Q, K, and V', visual_description=\"Three vertical column vectors appear side-by-side. The first is blue, labeled 'Q' for Query. The second is red, labeled 'K' for Key. The third is green, labeled 'V' for Value. Icons appear above them: a magnifying glass for Q, a barcode for K, and a treasure chest for V. A text label appears: 'Query: What I'm looking for', 'Key: What I contain', 'Value: What I contribute'.\", narration='Every word in a Transformer is represented by three vectors. The Query represents what a word is searching for. The Key acts like a label that other words can scan. And the Value is the actual information the word carries. The math of Attention is simply the process of matching these together.'), Scene(scene_type='mid', title='The Dot Product Comparison', visual_description=\"A blue vector (Q) and a red vector (K) are shown as arrows originating from the center of a 2D plane. Q is fixed at (2, 1). K begins to rotate. When K points in the same direction as Q, a text label 'Q times K' shows a large positive number (5.0). As K rotates away to be perpendicular, the number drops to 0. When it points in the opposite direction, it becomes -5.0. The formula 'Q times K-transpose' appears at the top.\", narration=\"We measure the 'relevance' between words using a dot product. If the Query and Key are similar—meaning they point in the same direction in high-dimensional space—their dot product is large. This represents a strong connection between the two words.\"), Scene(scene_type='mid', title='The Dimension Explosion', visual_description=\"The 2D plane expands into a 3D space, then collapses into a representation of a 128-dimensional vector (a long row of small numbers). Two of these 128-dimensional vectors are multiplied. The resulting dot product value is shown growing rapidly, jumping from 5 to 50 to 200. The text 'd-sub-k = 128' appears near the vectors. The number 200 begins to glow a harsh, bright red.\", narration='In modern models, these vectors can have hundreds or even thousands of dimensions. As the dimension, known as d-sub-k, grows, the dot products can become incredibly large. This might seem fine, but it creates a massive problem for the next step in our formula.'), Scene(scene_type='mid', title='The Softmax Wall', visual_description=\"A Softmax curve (an S-shaped curve) is plotted. On the x-axis, points are plotted based on our large dot product values. Because the values (like 200) are so large, they sit on the extreme right 'flat' part of the curve. A small vertical arrow representing a 'gradient' tries to move but stays at 0. The text 'Vanishing Gradients' appears in red. The animation pulses to show the model is 'stuck'.\", narration=\"We eventually pass these scores through a Softmax function to turn them into probabilities. But if the input values are too large, the Softmax curve becomes extremely flat. This leads to 'vanishing gradients,' where the model stops learning because the mathematical slope is effectively zero.\"), Scene(scene_type='mid', title='The Scaling Factor', visual_description=\"The formula 'Q times K-transpose' is shown. A division bar appears underneath it, and the symbol 'square root of d-sub-k' is placed in the denominator. We see the large dot product (200) being divided by the square root of 128 (approx 11.3). The result, 17.7, is much smaller. On the Softmax curve, the point moves from the flat extreme back toward the center where the slope is steep.\", narration='To fix this, we scale the dot product by dividing it by the square root of the dimension. This keeps the values in a range where the Softmax function stays sensitive, allowing the model to continue learning and refining its connections.'), Scene(scene_type='mid', title='Softmax to Weights', visual_description=\"A bar chart appears with 5 bars of different heights, representing raw scaled scores. As the word 'Softmax' scrolls over them, the bars transform: they all become positive, and their total height is normalized so they sum to exactly 1.0. One bar is significantly higher than the others, representing 0.8, while the others are 0.05 each.\", narration=\"The Softmax function then turns these scaled scores into weights. These are essentially probabilities that tell us exactly how much 'attention' to pay to each word. Everything now sums to one, giving us a clean distribution of importance.\"), Scene(scene_type='mid', title='The Final Weighted Sum', visual_description=\"The 5 bars from the previous scene (the weights) align vertically with 5 green 'Value' vectors (V). Each Value vector is scaled (stretched or squashed) by its corresponding weight. The 0.8 weight makes its Value vector look nearly full-sized, while the 0.05 weights make their Value vectors tiny. These scaled vectors then slide together and stack on top of each other to form one final 'Output' vector.\", narration='Finally, we multiply these weights by the Value vectors and sum them up. The result is a single vector that contains the most relevant information from across the entire sentence, filtered through the lens of our original Query.'), Scene(scene_type='mid', title='Hardware Optimization: FlashAttention', visual_description=\"The full attention matrix (a large grid) appears. It looks slow and heavy. Suddenly, the grid is sliced into small squares or 'tiles'. These tiles move quickly in and out of a small box labeled 'SRAM' (High-speed memory). The process looks much more fluid and rapid. The text 'FlashAttention: Tiling for Speed' appears.\", narration='While this math is elegant, calculating it for long sequences is computationally expensive. Modern engineering uses a technique called FlashAttention, which breaks the large matrix into smaller tiles. This allows GPUs to process the math without constantly moving data back and forth, making the whole operation significantly faster.'), Scene(scene_type='closing', title='The Complete Formula', visual_description=\"All the elements fly together to form the complete equation in the center of the screen: Attention of Q, K, and V equals the Softmax of Q times K-transpose over the square root of d-sub-k, all multiplied by V. The blue, red, and green colors are maintained. The background transitions back to the original sentence 'The bank of the river,' with the final Output vector glowing brightly above it.\", narration='This is Scaled Dot-Product Attention. By combining the search logic of Queries and Keys with the stability of a scaling factor, it provides the mathematical core that allows Transformers to understand context with incredible precision.')]),\n",
       " 'Multi-Head Attention: Seeing in Parallel': TopicStoryboard(topic_name='Multi-Head Attention: Seeing in Parallel', visual_concept='A single beam of white light (the input) passes through a prism, splitting into eight distinct colored rays (the heads). Each ray illuminates a different grammatical relationship in the same sentence simultaneously before being recombined into a single, richer beam of light.', scenes=[Scene(scene_type='hook', title='The Problem of the Weighted Average', visual_description=\"A sentence appears in the center: 'The chef cooked the meal that was delicious.' The word 'that' is highlighted in a glowing gold box. A single set of arrows emerges from 'that' and points toward 'chef' and 'meal'. A bar chart appears above 'that', showing a 50-50 split in attention between the two words. The word 'that' begins to blur, turning into a muddy, brownish-gray color as if the meanings of 'chef' and 'meal' are being mixed together into an indistinct soup.\", narration=\"In a simple attention mechanism, a word like 'that' has to decide what it's referring to. Is it the chef, or the meal? If we only use one attention filter, we often end up with a muddy average of both, losing the precise details of each relationship.\"), Scene(scene_type='mid', title='Enter the Multi-Head Prism', visual_description=\"The 'muddy' word 'that' is replaced by a crisp vector (a vertical column of 8 blocks). This vector moves toward a triangular prism labeled 'Linear Projections'. As it passes through, 8 smaller, distinct vectors emerge, each a different vibrant color (red, blue, green, etc.). They are arranged in a circular formation. Each vector is labeled 'Head 1' through 'Head 8'. Below them, the text 'Dimension = 64' appears for each head.\", narration=\"Multi-head attention solves this by splitting the input into several different perspectives. In the original Transformer paper, the model uses eight 'heads.' Each head is a smaller, specialized version of the attention process, operating in its own unique subspace.\"), Scene(scene_type='mid', title='Parallel Perspectives: Syntax vs. Semantics', visual_description=\"The screen splits into two panels. Left panel: 'Head 1' (Blue). The sentence 'The chef cooked the meal that was delicious' appears. A bold blue arrow connects 'that' to 'meal'. Right panel: 'Head 2' (Red). The same sentence appears, but a bold red arrow connects 'cooked' to 'chef'. The blue head is labeled 'Object Reference' and the red head is labeled 'Subject-Verb Relation'.\", narration=\"This allows the model to look at the sentence in parallel. While one head focuses on which noun a pronoun refers to, another can focus on the relationship between a verb and its subject. They don't have to compromise; they can both be right at the same time.\"), Scene(scene_type='mid', title='The Math of Scaling Down', visual_description=\"A large square matrix representing the original 512-dimensional space appears. It is sliced vertically into 8 thin rectangles, each 64 units wide. One of these thin rectangles zooms in. We see three labels appear: Query, Key, and Value. The numbers '512 divided by 8 equals 64' float above the slices.\", narration=\"To keep the computation efficient, we don't just copy the full model eight times. Instead, we project the original 512-dimensional data into 64-dimensional spaces. This ensures that the total work done by all eight heads combined is roughly the same as one giant attention head.\"), Scene(scene_type='mid', title='The Concatenation Step', visual_description=\"Eight small, colored output vectors (the results from each head) fly toward the center of the screen. They snap together vertically, forming one long, multicolored column. The label 'Concatenation' appears next to the stack. The stack is exactly the same height as the original input vector from the first scene.\", narration='Once each head has finished its work, we gather their individual insights and stack them back together. This process, called concatenation, creates a single vector that now contains a diverse set of information from all eight perspectives.'), Scene(scene_type='mid', title='The Final Projection', visual_description=\"The multicolored stack of vectors moves toward a large, gray weight matrix labeled 'W-sub-O'. As the stack passes through the matrix, the individual colors blend back into a single, solid gold color. This new vector looks like the original input but glows with a 'data aura' to indicate it is now more information-dense.\", narration=\"Finally, we pass this combined vector through one more linear projection. This acts like a final editor, blending the different heads' findings back into the model's main highway of information.\"), Scene(scene_type='mid', title='Optimizing for Speed: GQA and MQA', visual_description=\"A diagram of Multi-Head Attention appears on the left (8 Query, 8 Key, 8 Value heads). A new diagram appears on the right labeled 'Grouped-Query Attention'. It shows 8 Query heads (blue) but only 2 Key and Value heads (purple). Thin lines show multiple blue heads sharing one purple head. The label 'Less Memory' appears with a green upward arrow.\", narration='Modern models have taken this further with variations like Grouped-Query Attention. Since the Keys and Values take up the most memory during text generation, we can share them across multiple Query heads. This makes the model much faster and lighter without losing much accuracy.'), Scene(scene_type='closing', title='The Power of Many Eyes', visual_description=\"The screen zooms out to show a grid of many words from a paragraph. Thousands of tiny, multicolored lines flash between the words, representing the 8 heads working in parallel. The lines pulse in a rhythmic, organized fashion. The final text 'Multi-Head Attention: Clarity through Diversity' fades in over the animation.\", narration='By looking at the world through multiple lenses at once, Transformers can capture the rich, overlapping layers of human language. It’s this ability to see in parallel that makes them so remarkably good at understanding context.')]),\n",
       " 'Positional Encoding: Giving Order to Chaos': TopicStoryboard(topic_name='Positional Encoding: Giving Order to Chaos', visual_concept=\"The central metaphor is a 'Temporal Fingerprint.' We visualize words as points in high-dimensional space that are initially floating in a disordered cloud. We then apply a structured, rhythmic pattern of sine and cosine waves—like a rhythmic pulse across a keyboard—to anchor each word to a specific 'time' or 'place' without losing the ability to process them all at once.\", scenes=[Scene(scene_type='hook', title='The Bag of Words Problem', visual_description=\"Two sentences appear in the center of a dark screen: 'The dog bit the man' and 'The man bit the dog.' The words are represented as elegant, glowing blocks of text. Suddenly, the words break apart and fly into two identical glowing 'bags' or circular containers. The camera pans between the two bags, showing that inside, they look exactly the same—just a collection of the same five words floating randomly. A red 'X' appears between them to signify the loss of meaning.\", narration=\"In language, order is everything. But the Transformer model, unlike our brains or older AI, processes every word in a sentence simultaneously. To the model, 'The dog bit the man' looks identical to 'The man bit the dog.' It sees a bag of words, but it's blind to the sequence.\"), Scene(scene_type='mid', title='The Vector Addition', visual_description=\"A single word, 'Dog', is shown as a horizontal 1D array (a vector) of 10 colored squares, representing an embedding. Below it, a new vector of the same size appears, labeled 'Positional Encoding.' This second vector has a distinct gradient pattern. The two vectors move toward each other and merge vertically. The numbers inside the squares sum up (e.g., 0.5 + 0.1 becomes 0.6), resulting in a third 'Combined' vector. This combined vector now carries both the 'meaning' of the word and its 'location.'\", narration=\"To fix this, we need to give each word a unique signature based on its position. We do this by taking the word's embedding—a list of numbers representing its meaning—and simply adding it to a 'positional encoding' vector. This is a mathematical stamp that tells the model exactly where that word sits in line.\"), Scene(scene_type='mid', title='The Sinusoidal Blueprint', visual_description='A coordinate plane appears. A fast-moving sine wave (high frequency) is drawn in bright blue. Below it, a slower sine wave (medium frequency) is drawn in teal. Below that, a very slow sine wave (low frequency) is drawn in green. Vertical dashed lines drop down from the x-axis at integer positions 0, 1, 2, and 3. At each position, dots appear where the vertical line intersects the different waves. These dots form a vertical column of values for each position.', narration='But what should these stamps look like? The authors of the original Transformer paper used a clever trick involving sine and cosine waves. Imagine a series of waves, each vibrating at a different speed. For any given position in a sentence, we sample the height of these waves to create a unique code.'), Scene(scene_type='mid', title='The Encoding Matrix', visual_description=\"A large grid (matrix) emerges. The rows represent 'Position' (0 to 10) and the columns represent 'Model Dimension' (0 to 511). The grid is filled with a heat-map pattern: the left side oscillates rapidly between red and blue (high frequency), while the right side shows long, slow bands of color (low frequency). A single row is highlighted, showing how it captures a unique slice of all these different frequencies.\", narration='This creates a matrix of values where every row is a unique fingerprint. The first dimensions of the vector use high-frequency waves that change rapidly with every word. The later dimensions use low-frequency waves that change slowly, like the hour hand on a clock, providing a sense of the broader context.'), Scene(scene_type='mid', title='Relative Distance Logic', visual_description=\"Two words, 'The' and 'Cat', are shown at positions 1 and 4. The sine waves from the previous scene reappear. An arrow spans the distance between position 1 and 4. The math formula for the sine of (a+b) briefly flashes: 'sin(a + k) = sin(a)cos(k) + cos(a)sin(k)'. The visual shows that the relationship between the encoding at position 1 and position 4 is a predictable linear transformation, visualized as a rotating gear connecting the two points.\", narration=\"There's a beautiful mathematical reason for using sines and cosines. Because of trigonometric identities, the model can easily learn to calculate the relative distance between words. It doesn't just know that a word is at 'index five'; it can sense that another word is exactly 'three steps away,' regardless of how long the sentence is.\"), Scene(scene_type='mid', title='The Evolution to Rotary', visual_description=\"The 'Addition' visual from Scene 2 returns, but a 'Stop' sign appears over the plus sign. The scene transforms. A vector is shown as an arrow on a 2D plane. Instead of adding a second vector to it, the arrow physically rotates by an angle theta. The angle of rotation depends on the position of the word. This is labeled 'RoPE' (Rotary Positional Embedding). As the word 'Dog' moves from position 1 to position 5, the vector arrow spins further around the origin.\", narration=\"While adding waves works well, modern models like Llama or GPT-4 often use 'Rotary' embeddings. Instead of adding a signal, they rotate the word vectors in high-dimensional space. This 'spinning' approach makes it even easier for the model to handle massive context windows, sometimes up to hundreds of thousands of words.\"), Scene(scene_type='closing', title='Order Restored', visual_description=\"The 'bag of words' from the beginning returns. This time, each word block has a glowing, pulsing wave pattern inside it. The words move out of the bag and snap into a perfect, straight line. The sentence 'The dog bit the man' glows brightly. The camera pulls back to show thousands of these ordered lines forming a vast, structured library of information.\", narration=\"By injecting this sense of time into a parallel system, we get the best of both worlds: the blazing speed of the Transformer and the sequential precision of human language. Chaos is turned into order, allowing the model to finally understand the story we're trying to tell.\")]),\n",
       " 'Efficiency and Complexity: Why Transformers Scaled': TopicStoryboard(topic_name='Efficiency and Complexity: Why Transformers Scaled', visual_concept=\"The visualization uses a 'Factory vs. Flash' metaphor. Recurrent models are depicted as a slow, sequential assembly line where each step depends on the last, while Transformers are shown as a sudden flash of a grid where all computations ignite simultaneously. This contrast highlights why Transformers dominate on modern parallel hardware like GPUs, despite their high memory cost.\", scenes=[Scene(scene_type='hook', title='The Sequential Snail', visual_description=\"A sequence of five word-blocks ('The', 'cat', 'sat', 'on', 'mat') appears on a horizontal timeline. A small glowing 'state' orb moves slowly from left to right. It must fully enter 'The' and exit before it can move to 'cat'. As it moves, a progress bar at the bottom fills up at a sluggish, constant pace. Below this, a second timeline appears with the same words, but they all light up at the exact same moment with a bright golden flash.\", narration='In the early days of deep learning, we processed language like a relay race—one word at a time. But to build models that understand the whole internet, we needed to move away from waiting in line and start doing everything at once.'), Scene(scene_type='mid', title='The Complexity Table', visual_description=\"A clean, minimalist table fades in, based on the 'Attention is All You Need' paper. Rows are labeled 'Self-Attention' and 'Recurrent'. Columns are labeled 'Complexity per Layer' and 'Sequential Operations'. The cell for Self-Attention Complexity shows 'O(n squared times d)'. The cell for Recurrent shows 'O(n times d squared)'. 'n' is colored bright green (sequence length) and 'd' is colored cool blue (representation size).\", narration=\"To understand why Transformers won, we have to look at their computational 'rent.' Here, 'n' is the length of your text, and 'd' is how rich each word's representation is. At first glance, the Transformer's math looks more expensive because of that 'n squared' term.\"), Scene(scene_type='mid', title='Sequential vs Parallel', visual_description=\"The 'Sequential Operations' column of the table is highlighted. For Recurrent, it shows 'O(n)'. For Self-Attention, it shows 'O(1)'. The screen splits. On the left (RNN), a vertical stack of 'n' blocks must be processed one by one, like a tall ladder. On the right (Transformer), all 'n' blocks are spread out horizontally and a single 'compute' bar drops down, hitting all of them simultaneously.\", narration='But here is the game-changer: Sequential Operations. In an RNN, if you have a thousand words, you have a thousand steps that must happen one after another. In a Transformer, that number is effectively one. The math can be parallelized across the thousands of tiny cores inside a modern GPU.'), Scene(scene_type='mid', title='The Path Length Problem', visual_description=\"Five dots representing words are arranged in a line. For the 'Recurrent' view, arrows link them in a chain: 1 to 2, 2 to 3, 3 to 4. To get from word 1 to word 5, the signal must travel through four links. For the 'Transformer' view, the dots move into a circle, and every single dot is connected to every other dot by a direct, glowing line. The distance between any two points is always exactly one link.\", narration=\"There is also the 'Path Length.' In an RNN, a word at the start of a book has to travel through a long chain of memories to reach the end, losing detail along the way. In a Transformer, every word has a direct, high-speed connection to every other word, regardless of distance.\"), Scene(scene_type='mid', title='The Quadratic Achilles Heel', visual_description=\"A 10x10 grid representing attention scores between 10 tokens appears. It looks manageable. Suddenly, the number of tokens 'n' doubles to 20. The grid doesn't double; it grows four times larger to a 20x20. Then it doubles again to 40, and the grid explodes in size, filling the entire screen and pushing the boundaries until the lines become a dense, blurry mesh of pixels.\", narration=\"However, the Transformer has an Achilles heel. Because every word looks at every other word, the memory requirement grows quadratically. Double your document length, and you quadruple the memory needed. This 'n-squared' wall is why your browser might struggle with extremely long PDF summaries.\"), Scene(scene_type='mid', title='Breaking the Wall', visual_description=\"The dense 40x40 grid from the previous scene starts to fade. Only the diagonal and a few random spots stay lit—this is 'Sparse Attention'. Then, the grid is replaced by a sliding window that moves across the text—this is 'Linear Transformers'. Finally, a 'FlashAttention' logo appears, showing data moving efficiently between fast and slow memory modules (SRAM and HBM).\", narration=\"The AI community is currently obsessed with breaking this wall. Techniques like Sparse Attention or FlashAttention find clever ways to skip the unnecessary math, giving us the Transformer's speed without the quadratic cost.\"), Scene(scene_type='closing', title='The Scaling Victory', visual_description=\"A final side-by-side comparison. On the left, an RNN is slowly climbing a mountain of data. On the right, a Transformer is a rocket ship accelerating past it, fueled by 'Parallelism' and 'Direct Connections'. The text 'Efficiency = Scalability' appears in bold at the center.\", narration=\"Transformers didn't just win because they were smarter; they won because they were built to run fast on the hardware we actually have. By trading more total math for better parallel speed, they unlocked the era of massive scale.\")]),\n",
       " 'Results and Historical Significance': TopicStoryboard(topic_name='Results and Historical Significance', visual_concept=\"The visualization uses a 'Seismic Shift' metaphor. We begin with a rigid, slow-moving landscape of Recurrent Neural Networks, which is suddenly overtaken by a faster, more efficient architecture. The core visual elements include comparative bar charts that balance performance against cost, a blossoming 'family tree' of modern AI, and a literal timeline of progress that accelerates after 2017.\", scenes=[Scene(scene_type='hook', title='The 2017 Landscape', visual_description=\"The screen shows a dark, slate-grey background. A sequence of interlocking gear-like icons, labeled 'RNN' and 'LSTM', move slowly from left to right, representing the state of the art in 2017. A bright, gold spark appears in the center, labeled 'Attention Is All You Need'. As the spark expands, the gears begin to fade into the background, signaling the start of a new era.\", narration='In 2017, the AI world reached a turning point. For years, we thought processing language required a step-by-step, sequential approach. But a single paper was about to prove that we could do things much faster and much better.'), Scene(scene_type='mid', title='The BLEU Score Breakthrough', visual_description=\"A clean coordinate plane appears. The vertical axis is labeled 'BLEU Score (Translation Quality)'. Two bars rise: a silver bar for 'GNMT Ensemble' reaching 26.3 and a vibrant gold bar for 'Transformer (Big)' climbing to 28.4. A dashed line highlights the 2.1-point gap between them, with a 'plus 2.1' label appearing in the gap.\", narration=\"The results were immediate and shocking. On the standard English-to-German translation task, the 'Big' Transformer model didn't just win; it blew past the previous record-holders by over two full points. In the world of translation metrics, that is a massive leap forward.\"), Scene(scene_type='mid', title='Efficiency: The Cost of Brilliance', visual_description=\"The bar chart shrinks to the top left. On the right, two piles of golden coins appear. The pile for 'GNMT' is massive, towering off-screen. The pile for 'Transformer' is a tiny fraction—about one-tenth the size. Text appears: 'Training Cost: 2.3 times 10 to the 19th floating point operations'. The number for GNMT is shown to be significantly higher.\", narration=\"But the real shock wasn't just the quality—it was the price tag. The Transformer achieved these record-breaking results while using only a small fraction of the computing power required by its predecessors. It was both the smartest and the most efficient student in the room.\"), Scene(scene_type='mid', title='Proving Versatility', visual_description=\"The screen clears. A sentence appears: 'The quick brown fox jumps over the lazy dog.' Below it, the sentence transforms into a complex, branching tree structure, representing a 'Constituency Parse Tree'. The branches are drawn in glowing blue lines. A label 'English Constituency Parsing' fades in.\", narration='Critics wondered if this was just a translation trick. To prove them wrong, the researchers tested it on structural parsing—essentially diagramming the grammar of a sentence. The Transformer excelled there too, proving it had a deep, fundamental understanding of how language is built.'), Scene(scene_type='mid', title='The Death of Recurrence', visual_description=\"A long, winding chain of 'RNN' blocks stretches across the screen. A large, red 'X' mark stamps over the chain. The chain shatters into dust, and in its place, a parallel grid of 'Attention Layers' fades in. The words 'The Era of RNNs' transform into 'The Era of Transformers'.\", narration=\"This paper effectively ended the 'Era of Recurrent Networks.' It proved that recurrence—the idea of processing words one after another—wasn't just slow; it was unnecessary. The door was now wide open for parallel processing at a massive scale.\"), Scene(scene_type='mid', title='The Explosion of LLMs', visual_description=\"A horizontal timeline appears starting at 2017. At the 2017 mark, the 'Attention' paper sits as the root. In 2018, a node labeled 'BERT' branches out. In 2019, 'GPT-2' branches out in the opposite direction. By 2020 and beyond, the tree explodes into dozens of branches labeled 'GPT-3', 'T5', and 'Llama'.\", narration='Within just two years, the seeds planted by this architecture blossomed into the giants we know today. BERT and GPT-2 were direct descendants, using these same attention mechanisms to reach human-level performance on a huge variety of tasks.'), Scene(scene_type='mid', title='Modern Benchmarks', visual_description=\"The timeline fades. A new chart appears showing 'MMLU' and 'Human-Aligned' metrics. The old 'BLEU' score icon from earlier becomes a small, historical artifact in a museum display case. The background glows with a network of nodes representing modern Large Language Models.\", narration=\"Today, we've moved past simple translation scores to more complex benchmarks like MMLU, which test general knowledge and reasoning. While our metrics have evolved, the underlying engine—the Transformer—remains the gold standard for almost every modern AI.\"), Scene(scene_type='closing', title='A Monument of Research', visual_description=\"The screen displays a stylized image of the original paper's title page: 'Attention Is All You Need'. A 'Citation Count' counter starts at zero and rapidly spins up to a massive number in the tens of thousands. The paper icon turns into a golden monument as the camera zooms out to show it supporting the entire structure of modern AI.\", narration=\"With tens of thousands of citations, it is now the most influential paper in modern machine learning. It wasn't just a better way to translate text; it was the spark that ignited the current AI revolution. And it all started with the simple idea that attention is all you need.\")])}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "storyboards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cdd089a",
   "metadata": {},
   "source": [
    "# Animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abfdce9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "langchain_client = ChatGoogleGenerativeAI(\n",
    "    model=MODEL_NAME,\n",
    "    temperature=1.0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    ")\n",
    "langchain_client.client = gemini_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2ca54b2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=[{'type': 'text', 'text': 'Hello! How can I help you today?', 'extras': {'signature': 'CvgIAY89a1+b06Gv2pgcZdqk97aLZdnTFh8AekE0LzhnGtC5sB4HmxKPEJ9zONJkUVNUEfz3IP2d1UNADi8xUuGIjN8HUmeqXCna/wo3OadT9exNdD7w24GE24OkIlgbX/LdahlNUz2quFtJy/XI1yIGDDuodOnSfEFZraw9ZituLSr9Y29jpWfkEQUP69VOYX4F1Ed7JfwkkqDES6NimiUCn7AiVKn97e88wEN7hVCpj6VMwsALp6XVXHWTobttgFCW+9CgM6D1fVuaiRs1Tfq4VNsqnu2xaSvswd507adSBWTKMNBVx64NcbxClKskGEiiB4mQFleW7yCx9vy3nt9+ctI/CvkVA/CyjDqlhREaYunjetdc7cLF54/eWORnogZve8MZoOsuVpWIbx4EbcUW87XQlfnHsTx/QBcPQMvwaX/gPcX9K9oz2zKOt+GagrolGsOAHuAIZ78Fff3VFVtQzDx/6wbaZqXpOpnN1kK1LPgq43uUzfX9xteFVpNbbSMw8ZIhKNEO+fk8viFueHWCswfUzzgErEUNRv34GLlVW5No8qFoY2z7aNSGUByoJLCe1w5H9xh3Vz2P7OLXra8hvLw9+azE8KFr+RfVtiedKh40itbxalKYJ1tz+2h0RX5md0ERsKB+8LO4iXKkvCxLAHslaMboBV2Z98e0yCXWxaR+1u+T0ya/oDDb8C3XXdzUGjDYN+ktl2SOlR8lUGd0rkDDuHA1ZakkCUHn9sNAxQi4YI3wt+2bgys8BEjo6C/gtsdtFZuZ50dA0pQrk+4bq+tiT1fF7fj9ZIvG0FO88H4Cricn7poVNpioKYBcJ/FGvELOES0Dddmqn+dQTeJgNcj55dO8RUNfBgiek70bLabf0OoVsaFT6CWevNagtQLfByikryysD+rFyCRGjZio484i8iXKfnfmVxH2nw2Q3E4HY8W2u10F7yNRjZVaQmrDsnxRgAzr7knzWCnC0MdJ+UYc/I9E6QKVuZ6sT9JgJIQg5/lkIIeNOV2LRRyyqFiagkI80i5EmTX9/Np2iozT0Mab9TrWVYhMkRxjU5MWYHgQhtC0gl9F8IqZXtjVGXGuIitUVP2+xsAlKoUnaLouLw+u9x7Mu4aRnaEg4Fy5H9v6YyGr6FG7GNxzyqsGCnNvtKDUpLer2jUxPFuv3gKZfYVUNqpEug/PMJLNpzaH0tB6LXCuYi0wTYJJgKdsqXNAXWZkvWg1sTi3ekhGwB/ee1eVl2UH5Z3cLa/dj5YK94kz/T/kjau5FOXcnNJZWI+Tok1SUvUAwzedxfMhVGVDCiaHxR5E7vZA1LXex1u4lxcLw299IA3LYMX2clIKAZe32fJNMxpdQc5VpseKiTEKjzo4yrcPajURlPWrFoVsKe8XvLg/ZKQ6kqtGdwxq6MizsK68ygmHzPsrZ+WkO3hh4UMpZILFeBbAZcJA/G4wuLOvYmXXOqOitJoziyo8NyzFIujr0mKo6lKCemhnnh98GmpFDDRIkvdZit31+GL5X3xcHsygwjdtCw=='}}], additional_kwargs={}, response_metadata={'finish_reason': 'STOP', 'model_name': 'gemini-3-pro-preview', 'safety_ratings': [], 'model_provider': 'google_genai'}, id='lc_run--019b85b8-b51e-7d53-92d0-1d2abd763b43-0', usage_metadata={'input_tokens': 4, 'output_tokens': 293, 'total_tokens': 297, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 284}})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "langchain_client.invoke(\"Hello, world!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec57e7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "eduly_animation_client = EdulyAnimationClient(langchain_client, agent_workspace_path='./agent_workspace/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ebb4b33c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'The Shift from Recurrence to Attention': TopicStoryboard(topic_name='The Shift from Recurrence to Attention', visual_concept=\"The central metaphor is a 'Conveyor Belt vs. a Global Spotlight.' We visualize Recurrent Neural Networks (RNNs) as a linear, one-at-a-time assembly line that loses detail over distance, contrasted with Attention as a lightning-fast web where every part of a sentence connects to every other part simultaneously.\", scenes=[Scene(scene_type='hook', title='The Sequential Bottleneck', visual_description=\"A long sentence: 'The curious cat, which had spent all morning chasing a bright red laser pointer across the living room floor, finally fell asleep.' The words enter a narrow tube one by one. As each word enters, a small glowing box (the 'hidden state') moves to the next word. It looks slow and rhythmic, like a pulse. As the pulse reaches the end of the sentence, the glow from the word 'cat' at the beginning has almost completely faded away.\", narration='For years, teaching computers to understand language felt like watching a slow-moving assembly line. To process a sentence, AI models had to read one word at a time, carrying a tiny suitcase of memory from start to finish. But by the time they reached the end, that memory often became a blurry mess.'), Scene(scene_type='mid', title='The Mechanics of Recurrence', visual_description=\"We zoom into three words: 'The', 'curious', 'cat'. Below them, a math notation appears: h-sub-t = f(h-sub-t-minus-one, x-sub-t). An arrow goes from the 'The' box to the 'curious' box, carrying a vector of numbers. Then an arrow goes from 'curious' to 'cat'. The boxes are colored blue. The 'cat' box only knows about 'curious' because it was handed a message, not because it can see the word directly.\", narration=\"This is Recurrence. In models like LSTMs, each step depends entirely on the one before it. It’s a chain reaction. This creates two massive problems: it’s impossible to skip ahead, and the 'memory' of the beginning gets diluted as the chain grows longer.\"), Scene(scene_type='mid', title='The Training Wall', visual_description=\"The screen splits. On the left, a single GPU icon processes the 'conveyor belt' sentence slowly. A progress bar crawls. On the right, we see a vast empty space. Text appears: 'Idle Resources'. The sequential nature means we can't use more power to go faster, because step 100 must wait for step 99.\", narration=\"Because of this step-by-step nature, we couldn't just throw more hardware at the problem. Training was stuck in the slow lane, restricted by the speed of the sequence itself, no matter how many GPUs we had.\"), Scene(scene_type='mid', title='Attention is All You Need', visual_description=\"The conveyor belt tube shatters. The sentence 'The cat ... fell asleep' is now laid out in a circle. Suddenly, lines of light flash simultaneously between every single word. The word 'asleep' lights up, and bright gold lines connect it directly back to 'cat' and 'curious', bypassing everything in between.\", narration='Then, in 2017, a landmark paper proposed a radical shift: Attention. What if we stopped the chain and just let every word look at every other word at the exact same time? No more waiting, and no more distance.'), Scene(scene_type='mid', title='The Power of the Matrix', visual_description=\"The circular connections transform into a 15x15 grid (an Attention Map). The rows and columns are labeled with the words of our sentence. Cells where words relate (like 'cat' and 'asleep') glow intensely bright. The entire grid populates with colors all at once, rather than one cell at a time.\", narration='In an Attention-based model, like the Transformer, we calculate all these relationships using matrix multiplication. This is a game-changer because GPUs are specifically designed to do these massive math blocks in parallel, instantly.'), Scene(scene_type='mid', title='The Scaling Revolution', visual_description=\"We zoom out to see thousands of these grids stacking on top of each other. A graph appears showing 'Model Size' on the X-axis and 'Capability' on the Y-axis. The line for RNNs plateaus early, but the line for Transformers shoots upward exponentially. Icons for GPT-1, GPT-2, GPT-3, and GPT-4 appear along the rising curve.\", narration=\"Removing the sequential bottleneck allowed us to scale. We could train on the entire internet and build models with billions of parameters. This shift is exactly what sparked the modern AI explosion we're seeing today.\"), Scene(scene_type='mid', title='The Trade-off: Context Cost', visual_description=\"The 15x15 grid grows to 1000x1000, then 10,000x10,000. As it grows, it starts to glow red and then 'crack' under its own weight. A label appears: 'Computational Cost = Length Squared'. The screen shows that as sequences get very long, the memory needed to store every connection becomes overwhelming.\", narration=\"But there's a catch. While Attention is fast, it's also hungry. Since every word looks at every other word, doubling the sentence length quadruples the work. This 'quadratic cost' makes processing entire books very difficult for standard Transformers.\"), Scene(scene_type='closing', title='Full Circle: The New Recurrence', visual_description=\"A new architecture appears, labeled 'Mamba / SSM'. It looks like a hybrid: it has the sleek, fast flow of the conveyor belt but the high-tech glow of Attention. It moves linearly like an RNN but doesn't lose the color/memory of the first word. The 'cat' stays bright gold all the way to the end.\", narration=\"Interestingly, the story isn't over. New models, like Mamba, are trying to bring back the efficiency of recurrence while keeping the long-term memory of attention. We're finding that the best way forward might be a smarter way to remember the past.\")]),\n",
       " 'The Encoder-Decoder Structure': TopicStoryboard(topic_name='The Encoder-Decoder Structure', visual_concept=\"A 'Two-Tower' architectural metaphor where the Encoder tower acts as an 'Understanding Engine' that converts raw text into a glowing field of context, and the Decoder tower acts as a 'Generation Engine' that reaches into that field to weave a new sequence word-by-word.\", scenes=[Scene(scene_type='hook', title='The Bridge of Translation', visual_description=\"On a dark blue background, the English sentence 'The cat sat on the mat' appears in a clean white font on the left. On the right, the German translation 'Die Katze saß auf der Matte' appears. Between them, two vertical grey rectangles (the 'Towers') emerge. A bridge of golden light pulses between them, connecting the input to the output. The camera pans out to show the symmetry of the two towers.\", narration='How does a machine take a thought in one language and perfectly reconstruct it in another? The secret lies in a dual architecture known as the Encoder and the Decoder. It is a two-tower system designed to first understand, and then create.'), Scene(scene_type='mid', title=\"The Encoder's Layers\", visual_description=\"The left 'Encoder' tower zooms in. We see it's actually a stack of 6 identical blocks labeled 'Layer 1' through 'Layer 6'. The word 'The' enters the bottom of Layer 1 as a simple vector (a small row of 8 colored squares). As it passes through each layer, the vector pulses and changes colors, becoming more complex. By the time it reaches Layer 6, the vector is surrounded by a faint glowing aura.\", narration='The Encoder is the understanding engine. It is made of six identical layers stacked on top of each other. As an input sentence passes through this stack, the model processes the raw words into a sequence of deep, mathematical representations that capture the meaning and context of every word.'), Scene(scene_type='mid', title='Inside the Encoder Layer', visual_description=\"We zoom into a single Encoder block. Inside, we see two distinct stages. The first is a box labeled 'Self-Attention' and the second is 'Feed-Forward'. An arrow shows the data flowing into Self-Attention, then into Feed-Forward. A 'Residual Connection'—a curved arrow—loops around each stage, bypassing the box and merging back into the main line.\", narration=\"Each of these six layers has two main jobs. First, self-attention allows the words to 'talk' to each other to figure out their relationships. Then, a feed-forward network refines that information. These layers are held together by residual connections, which act like express lanes to keep the gradient flowing smoothly during learning.\"), Scene(scene_type='mid', title='The Contextual Cloud', visual_description=\"The Encoder tower finishes processing. The top of the tower now holds a set of six glowing vectors, representing the fully 'understood' sentence. These vectors float in a space between the two towers, creating a 'Contextual Cloud'. Small lines of light shimmer within the cloud, indicating the rich relationships between the words 'cat', 'sat', and 'mat'.\", narration=\"By the time the information leaves the Encoder, it is no longer just a list of words. It is a rich, continuous representation—a sort of 'mental map' of the entire input sentence that the Decoder can now use.\"), Scene(scene_type='mid', title=\"The Decoder's First Move\", visual_description=\"The right 'Decoder' tower activates. It is also a stack of 6 layers. At the bottom, a special 'Start' token enters. Unlike the Encoder which sees everything at once, the Decoder is mostly empty. A single word, 'Die', is generated at the top of the tower and then fed back into the bottom for the next step.\", narration='The Decoder is the generation engine. It works one step at a time, in what we call an auto-regressive process. It starts with a blank slate and generates the first word of the output. But how does it know what to write?'), Scene(scene_type='mid', title='The Encoder-Decoder Attention Bridge', visual_description=\"Inside a Decoder layer, three stages appear: 'Masked Self-Attention', then a new middle stage called 'Encoder-Decoder Attention', then 'Feed-Forward'. A beam of light shoots from the 'Encoder-Decoder Attention' box over to the glowing Contextual Cloud created by the Encoder. It 'grabs' information about the English word 'The' to help generate the German word 'Die'.\", narration=\"This is the magic moment. The Decoder has a third sub-layer called Encoder-Decoder attention. This allows the decoder to 'look back' at the Encoder’s map. While generating each German word, it can focus specifically on the most relevant English words to ensure the translation is accurate.\"), Scene(scene_type='mid', title='Stabilizing the Flow', visual_description=\"A close-up of the boundary between layers. We see a 'Layer Normalization' bar. The vectors passing through are 'squashed' and centered, keeping their values within a specific range (indicated by a small graph showing a bell curve). The residual connection arrows from earlier are highlighted in bright yellow.\", narration='Because these towers are deep, they use Layer Normalization after every step. Think of this as a stabilizer that keeps the mathematical values from exploding or shrinking too much, ensuring the model stays focused and trainable.'), Scene(scene_type='mid', title='The Modern Divergence', visual_description=\"The two towers separate. On the left, the Encoder tower stands alone under the label 'BERT'. On the right, the Decoder tower stands alone under the label 'GPT-4'. In the middle, the original connected two-tower structure is labeled 'T5 / Original Transformer'.\", narration=\"While the original Transformer used both towers for translation, modern AI has branched out. Models like BERT use only the Encoder for understanding tasks, while famous models like GPT-4 are 'Decoder-only,' optimized specifically for generating fluent text one word at a time.\"), Scene(scene_type='closing', title='The Symphony of Sequence', visual_description=\"The full Encoder-Decoder structure is shown again. We see the English sentence flow in, turn into the glowing cloud, and the German sentence emerge word-by-word on the right. The entire system pulses with a rhythmic light. The text 'Efficiency + Context' appears at the bottom.\", narration='The Encoder-Decoder structure changed everything by allowing machines to process sequences with both global context and surgical precision. Whether it is translating languages or writing code, this two-tower dance is the foundation of modern intelligence.')]),\n",
       " 'Scaled Dot-Product Attention: The Mathematical Core': TopicStoryboard(topic_name='Scaled Dot-Product Attention: The Mathematical Core', visual_concept=\"A library search system where 'Search Queries' (Blue) are compared against 'Book Labels' (Red) to extract the most relevant 'Knowledge' (Green). The central visual is the 'Attention Formula' which acts as a lens, focusing light from many sources onto a single point of understanding.\", scenes=[Scene(scene_type='hook', title='The Ambiguity Problem', visual_description=\"On a dark background, the sentence 'The bank of the river' appears in white. The word 'bank' is highlighted with a soft yellow glow. Below it, another sentence appears: 'The bank gave me a loan.' Both words 'bank' are identical. Suddenly, a blue arrow labeled 'Query' emerges from the word 'bank' in the first sentence. It scans the word 'river' which glows red, labeled 'Key'. A connection is made, and the word 'bank' turns a watery blue. In the second sentence, the 'bank' Query scans 'loan' (Key), and the 'bank' turns a money-green. The background fades to a coordinate plane.\", narration=\"To understand a sentence, a computer needs to know which words relate to each other. A word like 'bank' changes meaning based on its neighbors. To solve this, Transformers use a mechanism called Attention to decide which words to focus on. It is essentially a sophisticated search-and-retrieval system.\"), Scene(scene_type='mid', title='Defining Q, K, and V', visual_description=\"Three vertical column vectors appear side-by-side. The first is blue, labeled 'Q' for Query. The second is red, labeled 'K' for Key. The third is green, labeled 'V' for Value. Icons appear above them: a magnifying glass for Q, a barcode for K, and a treasure chest for V. A text label appears: 'Query: What I'm looking for', 'Key: What I contain', 'Value: What I contribute'.\", narration='Every word in a Transformer is represented by three vectors. The Query represents what a word is searching for. The Key acts like a label that other words can scan. And the Value is the actual information the word carries. The math of Attention is simply the process of matching these together.'), Scene(scene_type='mid', title='The Dot Product Comparison', visual_description=\"A blue vector (Q) and a red vector (K) are shown as arrows originating from the center of a 2D plane. Q is fixed at (2, 1). K begins to rotate. When K points in the same direction as Q, a text label 'Q times K' shows a large positive number (5.0). As K rotates away to be perpendicular, the number drops to 0. When it points in the opposite direction, it becomes -5.0. The formula 'Q times K-transpose' appears at the top.\", narration=\"We measure the 'relevance' between words using a dot product. If the Query and Key are similar—meaning they point in the same direction in high-dimensional space—their dot product is large. This represents a strong connection between the two words.\"), Scene(scene_type='mid', title='The Dimension Explosion', visual_description=\"The 2D plane expands into a 3D space, then collapses into a representation of a 128-dimensional vector (a long row of small numbers). Two of these 128-dimensional vectors are multiplied. The resulting dot product value is shown growing rapidly, jumping from 5 to 50 to 200. The text 'd-sub-k = 128' appears near the vectors. The number 200 begins to glow a harsh, bright red.\", narration='In modern models, these vectors can have hundreds or even thousands of dimensions. As the dimension, known as d-sub-k, grows, the dot products can become incredibly large. This might seem fine, but it creates a massive problem for the next step in our formula.'), Scene(scene_type='mid', title='The Softmax Wall', visual_description=\"A Softmax curve (an S-shaped curve) is plotted. On the x-axis, points are plotted based on our large dot product values. Because the values (like 200) are so large, they sit on the extreme right 'flat' part of the curve. A small vertical arrow representing a 'gradient' tries to move but stays at 0. The text 'Vanishing Gradients' appears in red. The animation pulses to show the model is 'stuck'.\", narration=\"We eventually pass these scores through a Softmax function to turn them into probabilities. But if the input values are too large, the Softmax curve becomes extremely flat. This leads to 'vanishing gradients,' where the model stops learning because the mathematical slope is effectively zero.\"), Scene(scene_type='mid', title='The Scaling Factor', visual_description=\"The formula 'Q times K-transpose' is shown. A division bar appears underneath it, and the symbol 'square root of d-sub-k' is placed in the denominator. We see the large dot product (200) being divided by the square root of 128 (approx 11.3). The result, 17.7, is much smaller. On the Softmax curve, the point moves from the flat extreme back toward the center where the slope is steep.\", narration='To fix this, we scale the dot product by dividing it by the square root of the dimension. This keeps the values in a range where the Softmax function stays sensitive, allowing the model to continue learning and refining its connections.'), Scene(scene_type='mid', title='Softmax to Weights', visual_description=\"A bar chart appears with 5 bars of different heights, representing raw scaled scores. As the word 'Softmax' scrolls over them, the bars transform: they all become positive, and their total height is normalized so they sum to exactly 1.0. One bar is significantly higher than the others, representing 0.8, while the others are 0.05 each.\", narration=\"The Softmax function then turns these scaled scores into weights. These are essentially probabilities that tell us exactly how much 'attention' to pay to each word. Everything now sums to one, giving us a clean distribution of importance.\"), Scene(scene_type='mid', title='The Final Weighted Sum', visual_description=\"The 5 bars from the previous scene (the weights) align vertically with 5 green 'Value' vectors (V). Each Value vector is scaled (stretched or squashed) by its corresponding weight. The 0.8 weight makes its Value vector look nearly full-sized, while the 0.05 weights make their Value vectors tiny. These scaled vectors then slide together and stack on top of each other to form one final 'Output' vector.\", narration='Finally, we multiply these weights by the Value vectors and sum them up. The result is a single vector that contains the most relevant information from across the entire sentence, filtered through the lens of our original Query.'), Scene(scene_type='mid', title='Hardware Optimization: FlashAttention', visual_description=\"The full attention matrix (a large grid) appears. It looks slow and heavy. Suddenly, the grid is sliced into small squares or 'tiles'. These tiles move quickly in and out of a small box labeled 'SRAM' (High-speed memory). The process looks much more fluid and rapid. The text 'FlashAttention: Tiling for Speed' appears.\", narration='While this math is elegant, calculating it for long sequences is computationally expensive. Modern engineering uses a technique called FlashAttention, which breaks the large matrix into smaller tiles. This allows GPUs to process the math without constantly moving data back and forth, making the whole operation significantly faster.'), Scene(scene_type='closing', title='The Complete Formula', visual_description=\"All the elements fly together to form the complete equation in the center of the screen: Attention of Q, K, and V equals the Softmax of Q times K-transpose over the square root of d-sub-k, all multiplied by V. The blue, red, and green colors are maintained. The background transitions back to the original sentence 'The bank of the river,' with the final Output vector glowing brightly above it.\", narration='This is Scaled Dot-Product Attention. By combining the search logic of Queries and Keys with the stability of a scaling factor, it provides the mathematical core that allows Transformers to understand context with incredible precision.')]),\n",
       " 'Multi-Head Attention: Seeing in Parallel': TopicStoryboard(topic_name='Multi-Head Attention: Seeing in Parallel', visual_concept='A single beam of white light (the input) passes through a prism, splitting into eight distinct colored rays (the heads). Each ray illuminates a different grammatical relationship in the same sentence simultaneously before being recombined into a single, richer beam of light.', scenes=[Scene(scene_type='hook', title='The Problem of the Weighted Average', visual_description=\"A sentence appears in the center: 'The chef cooked the meal that was delicious.' The word 'that' is highlighted in a glowing gold box. A single set of arrows emerges from 'that' and points toward 'chef' and 'meal'. A bar chart appears above 'that', showing a 50-50 split in attention between the two words. The word 'that' begins to blur, turning into a muddy, brownish-gray color as if the meanings of 'chef' and 'meal' are being mixed together into an indistinct soup.\", narration=\"In a simple attention mechanism, a word like 'that' has to decide what it's referring to. Is it the chef, or the meal? If we only use one attention filter, we often end up with a muddy average of both, losing the precise details of each relationship.\"), Scene(scene_type='mid', title='Enter the Multi-Head Prism', visual_description=\"The 'muddy' word 'that' is replaced by a crisp vector (a vertical column of 8 blocks). This vector moves toward a triangular prism labeled 'Linear Projections'. As it passes through, 8 smaller, distinct vectors emerge, each a different vibrant color (red, blue, green, etc.). They are arranged in a circular formation. Each vector is labeled 'Head 1' through 'Head 8'. Below them, the text 'Dimension = 64' appears for each head.\", narration=\"Multi-head attention solves this by splitting the input into several different perspectives. In the original Transformer paper, the model uses eight 'heads.' Each head is a smaller, specialized version of the attention process, operating in its own unique subspace.\"), Scene(scene_type='mid', title='Parallel Perspectives: Syntax vs. Semantics', visual_description=\"The screen splits into two panels. Left panel: 'Head 1' (Blue). The sentence 'The chef cooked the meal that was delicious' appears. A bold blue arrow connects 'that' to 'meal'. Right panel: 'Head 2' (Red). The same sentence appears, but a bold red arrow connects 'cooked' to 'chef'. The blue head is labeled 'Object Reference' and the red head is labeled 'Subject-Verb Relation'.\", narration=\"This allows the model to look at the sentence in parallel. While one head focuses on which noun a pronoun refers to, another can focus on the relationship between a verb and its subject. They don't have to compromise; they can both be right at the same time.\"), Scene(scene_type='mid', title='The Math of Scaling Down', visual_description=\"A large square matrix representing the original 512-dimensional space appears. It is sliced vertically into 8 thin rectangles, each 64 units wide. One of these thin rectangles zooms in. We see three labels appear: Query, Key, and Value. The numbers '512 divided by 8 equals 64' float above the slices.\", narration=\"To keep the computation efficient, we don't just copy the full model eight times. Instead, we project the original 512-dimensional data into 64-dimensional spaces. This ensures that the total work done by all eight heads combined is roughly the same as one giant attention head.\"), Scene(scene_type='mid', title='The Concatenation Step', visual_description=\"Eight small, colored output vectors (the results from each head) fly toward the center of the screen. They snap together vertically, forming one long, multicolored column. The label 'Concatenation' appears next to the stack. The stack is exactly the same height as the original input vector from the first scene.\", narration='Once each head has finished its work, we gather their individual insights and stack them back together. This process, called concatenation, creates a single vector that now contains a diverse set of information from all eight perspectives.'), Scene(scene_type='mid', title='The Final Projection', visual_description=\"The multicolored stack of vectors moves toward a large, gray weight matrix labeled 'W-sub-O'. As the stack passes through the matrix, the individual colors blend back into a single, solid gold color. This new vector looks like the original input but glows with a 'data aura' to indicate it is now more information-dense.\", narration=\"Finally, we pass this combined vector through one more linear projection. This acts like a final editor, blending the different heads' findings back into the model's main highway of information.\"), Scene(scene_type='mid', title='Optimizing for Speed: GQA and MQA', visual_description=\"A diagram of Multi-Head Attention appears on the left (8 Query, 8 Key, 8 Value heads). A new diagram appears on the right labeled 'Grouped-Query Attention'. It shows 8 Query heads (blue) but only 2 Key and Value heads (purple). Thin lines show multiple blue heads sharing one purple head. The label 'Less Memory' appears with a green upward arrow.\", narration='Modern models have taken this further with variations like Grouped-Query Attention. Since the Keys and Values take up the most memory during text generation, we can share them across multiple Query heads. This makes the model much faster and lighter without losing much accuracy.'), Scene(scene_type='closing', title='The Power of Many Eyes', visual_description=\"The screen zooms out to show a grid of many words from a paragraph. Thousands of tiny, multicolored lines flash between the words, representing the 8 heads working in parallel. The lines pulse in a rhythmic, organized fashion. The final text 'Multi-Head Attention: Clarity through Diversity' fades in over the animation.\", narration='By looking at the world through multiple lenses at once, Transformers can capture the rich, overlapping layers of human language. It’s this ability to see in parallel that makes them so remarkably good at understanding context.')]),\n",
       " 'Positional Encoding: Giving Order to Chaos': TopicStoryboard(topic_name='Positional Encoding: Giving Order to Chaos', visual_concept=\"The central metaphor is a 'Temporal Fingerprint.' We visualize words as points in high-dimensional space that are initially floating in a disordered cloud. We then apply a structured, rhythmic pattern of sine and cosine waves—like a rhythmic pulse across a keyboard—to anchor each word to a specific 'time' or 'place' without losing the ability to process them all at once.\", scenes=[Scene(scene_type='hook', title='The Bag of Words Problem', visual_description=\"Two sentences appear in the center of a dark screen: 'The dog bit the man' and 'The man bit the dog.' The words are represented as elegant, glowing blocks of text. Suddenly, the words break apart and fly into two identical glowing 'bags' or circular containers. The camera pans between the two bags, showing that inside, they look exactly the same—just a collection of the same five words floating randomly. A red 'X' appears between them to signify the loss of meaning.\", narration=\"In language, order is everything. But the Transformer model, unlike our brains or older AI, processes every word in a sentence simultaneously. To the model, 'The dog bit the man' looks identical to 'The man bit the dog.' It sees a bag of words, but it's blind to the sequence.\"), Scene(scene_type='mid', title='The Vector Addition', visual_description=\"A single word, 'Dog', is shown as a horizontal 1D array (a vector) of 10 colored squares, representing an embedding. Below it, a new vector of the same size appears, labeled 'Positional Encoding.' This second vector has a distinct gradient pattern. The two vectors move toward each other and merge vertically. The numbers inside the squares sum up (e.g., 0.5 + 0.1 becomes 0.6), resulting in a third 'Combined' vector. This combined vector now carries both the 'meaning' of the word and its 'location.'\", narration=\"To fix this, we need to give each word a unique signature based on its position. We do this by taking the word's embedding—a list of numbers representing its meaning—and simply adding it to a 'positional encoding' vector. This is a mathematical stamp that tells the model exactly where that word sits in line.\"), Scene(scene_type='mid', title='The Sinusoidal Blueprint', visual_description='A coordinate plane appears. A fast-moving sine wave (high frequency) is drawn in bright blue. Below it, a slower sine wave (medium frequency) is drawn in teal. Below that, a very slow sine wave (low frequency) is drawn in green. Vertical dashed lines drop down from the x-axis at integer positions 0, 1, 2, and 3. At each position, dots appear where the vertical line intersects the different waves. These dots form a vertical column of values for each position.', narration='But what should these stamps look like? The authors of the original Transformer paper used a clever trick involving sine and cosine waves. Imagine a series of waves, each vibrating at a different speed. For any given position in a sentence, we sample the height of these waves to create a unique code.'), Scene(scene_type='mid', title='The Encoding Matrix', visual_description=\"A large grid (matrix) emerges. The rows represent 'Position' (0 to 10) and the columns represent 'Model Dimension' (0 to 511). The grid is filled with a heat-map pattern: the left side oscillates rapidly between red and blue (high frequency), while the right side shows long, slow bands of color (low frequency). A single row is highlighted, showing how it captures a unique slice of all these different frequencies.\", narration='This creates a matrix of values where every row is a unique fingerprint. The first dimensions of the vector use high-frequency waves that change rapidly with every word. The later dimensions use low-frequency waves that change slowly, like the hour hand on a clock, providing a sense of the broader context.'), Scene(scene_type='mid', title='Relative Distance Logic', visual_description=\"Two words, 'The' and 'Cat', are shown at positions 1 and 4. The sine waves from the previous scene reappear. An arrow spans the distance between position 1 and 4. The math formula for the sine of (a+b) briefly flashes: 'sin(a + k) = sin(a)cos(k) + cos(a)sin(k)'. The visual shows that the relationship between the encoding at position 1 and position 4 is a predictable linear transformation, visualized as a rotating gear connecting the two points.\", narration=\"There's a beautiful mathematical reason for using sines and cosines. Because of trigonometric identities, the model can easily learn to calculate the relative distance between words. It doesn't just know that a word is at 'index five'; it can sense that another word is exactly 'three steps away,' regardless of how long the sentence is.\"), Scene(scene_type='mid', title='The Evolution to Rotary', visual_description=\"The 'Addition' visual from Scene 2 returns, but a 'Stop' sign appears over the plus sign. The scene transforms. A vector is shown as an arrow on a 2D plane. Instead of adding a second vector to it, the arrow physically rotates by an angle theta. The angle of rotation depends on the position of the word. This is labeled 'RoPE' (Rotary Positional Embedding). As the word 'Dog' moves from position 1 to position 5, the vector arrow spins further around the origin.\", narration=\"While adding waves works well, modern models like Llama or GPT-4 often use 'Rotary' embeddings. Instead of adding a signal, they rotate the word vectors in high-dimensional space. This 'spinning' approach makes it even easier for the model to handle massive context windows, sometimes up to hundreds of thousands of words.\"), Scene(scene_type='closing', title='Order Restored', visual_description=\"The 'bag of words' from the beginning returns. This time, each word block has a glowing, pulsing wave pattern inside it. The words move out of the bag and snap into a perfect, straight line. The sentence 'The dog bit the man' glows brightly. The camera pulls back to show thousands of these ordered lines forming a vast, structured library of information.\", narration=\"By injecting this sense of time into a parallel system, we get the best of both worlds: the blazing speed of the Transformer and the sequential precision of human language. Chaos is turned into order, allowing the model to finally understand the story we're trying to tell.\")]),\n",
       " 'Efficiency and Complexity: Why Transformers Scaled': TopicStoryboard(topic_name='Efficiency and Complexity: Why Transformers Scaled', visual_concept=\"The visualization uses a 'Factory vs. Flash' metaphor. Recurrent models are depicted as a slow, sequential assembly line where each step depends on the last, while Transformers are shown as a sudden flash of a grid where all computations ignite simultaneously. This contrast highlights why Transformers dominate on modern parallel hardware like GPUs, despite their high memory cost.\", scenes=[Scene(scene_type='hook', title='The Sequential Snail', visual_description=\"A sequence of five word-blocks ('The', 'cat', 'sat', 'on', 'mat') appears on a horizontal timeline. A small glowing 'state' orb moves slowly from left to right. It must fully enter 'The' and exit before it can move to 'cat'. As it moves, a progress bar at the bottom fills up at a sluggish, constant pace. Below this, a second timeline appears with the same words, but they all light up at the exact same moment with a bright golden flash.\", narration='In the early days of deep learning, we processed language like a relay race—one word at a time. But to build models that understand the whole internet, we needed to move away from waiting in line and start doing everything at once.'), Scene(scene_type='mid', title='The Complexity Table', visual_description=\"A clean, minimalist table fades in, based on the 'Attention is All You Need' paper. Rows are labeled 'Self-Attention' and 'Recurrent'. Columns are labeled 'Complexity per Layer' and 'Sequential Operations'. The cell for Self-Attention Complexity shows 'O(n squared times d)'. The cell for Recurrent shows 'O(n times d squared)'. 'n' is colored bright green (sequence length) and 'd' is colored cool blue (representation size).\", narration=\"To understand why Transformers won, we have to look at their computational 'rent.' Here, 'n' is the length of your text, and 'd' is how rich each word's representation is. At first glance, the Transformer's math looks more expensive because of that 'n squared' term.\"), Scene(scene_type='mid', title='Sequential vs Parallel', visual_description=\"The 'Sequential Operations' column of the table is highlighted. For Recurrent, it shows 'O(n)'. For Self-Attention, it shows 'O(1)'. The screen splits. On the left (RNN), a vertical stack of 'n' blocks must be processed one by one, like a tall ladder. On the right (Transformer), all 'n' blocks are spread out horizontally and a single 'compute' bar drops down, hitting all of them simultaneously.\", narration='But here is the game-changer: Sequential Operations. In an RNN, if you have a thousand words, you have a thousand steps that must happen one after another. In a Transformer, that number is effectively one. The math can be parallelized across the thousands of tiny cores inside a modern GPU.'), Scene(scene_type='mid', title='The Path Length Problem', visual_description=\"Five dots representing words are arranged in a line. For the 'Recurrent' view, arrows link them in a chain: 1 to 2, 2 to 3, 3 to 4. To get from word 1 to word 5, the signal must travel through four links. For the 'Transformer' view, the dots move into a circle, and every single dot is connected to every other dot by a direct, glowing line. The distance between any two points is always exactly one link.\", narration=\"There is also the 'Path Length.' In an RNN, a word at the start of a book has to travel through a long chain of memories to reach the end, losing detail along the way. In a Transformer, every word has a direct, high-speed connection to every other word, regardless of distance.\"), Scene(scene_type='mid', title='The Quadratic Achilles Heel', visual_description=\"A 10x10 grid representing attention scores between 10 tokens appears. It looks manageable. Suddenly, the number of tokens 'n' doubles to 20. The grid doesn't double; it grows four times larger to a 20x20. Then it doubles again to 40, and the grid explodes in size, filling the entire screen and pushing the boundaries until the lines become a dense, blurry mesh of pixels.\", narration=\"However, the Transformer has an Achilles heel. Because every word looks at every other word, the memory requirement grows quadratically. Double your document length, and you quadruple the memory needed. This 'n-squared' wall is why your browser might struggle with extremely long PDF summaries.\"), Scene(scene_type='mid', title='Breaking the Wall', visual_description=\"The dense 40x40 grid from the previous scene starts to fade. Only the diagonal and a few random spots stay lit—this is 'Sparse Attention'. Then, the grid is replaced by a sliding window that moves across the text—this is 'Linear Transformers'. Finally, a 'FlashAttention' logo appears, showing data moving efficiently between fast and slow memory modules (SRAM and HBM).\", narration=\"The AI community is currently obsessed with breaking this wall. Techniques like Sparse Attention or FlashAttention find clever ways to skip the unnecessary math, giving us the Transformer's speed without the quadratic cost.\"), Scene(scene_type='closing', title='The Scaling Victory', visual_description=\"A final side-by-side comparison. On the left, an RNN is slowly climbing a mountain of data. On the right, a Transformer is a rocket ship accelerating past it, fueled by 'Parallelism' and 'Direct Connections'. The text 'Efficiency = Scalability' appears in bold at the center.\", narration=\"Transformers didn't just win because they were smarter; they won because they were built to run fast on the hardware we actually have. By trading more total math for better parallel speed, they unlocked the era of massive scale.\")]),\n",
       " 'Results and Historical Significance': TopicStoryboard(topic_name='Results and Historical Significance', visual_concept=\"The visualization uses a 'Seismic Shift' metaphor. We begin with a rigid, slow-moving landscape of Recurrent Neural Networks, which is suddenly overtaken by a faster, more efficient architecture. The core visual elements include comparative bar charts that balance performance against cost, a blossoming 'family tree' of modern AI, and a literal timeline of progress that accelerates after 2017.\", scenes=[Scene(scene_type='hook', title='The 2017 Landscape', visual_description=\"The screen shows a dark, slate-grey background. A sequence of interlocking gear-like icons, labeled 'RNN' and 'LSTM', move slowly from left to right, representing the state of the art in 2017. A bright, gold spark appears in the center, labeled 'Attention Is All You Need'. As the spark expands, the gears begin to fade into the background, signaling the start of a new era.\", narration='In 2017, the AI world reached a turning point. For years, we thought processing language required a step-by-step, sequential approach. But a single paper was about to prove that we could do things much faster and much better.'), Scene(scene_type='mid', title='The BLEU Score Breakthrough', visual_description=\"A clean coordinate plane appears. The vertical axis is labeled 'BLEU Score (Translation Quality)'. Two bars rise: a silver bar for 'GNMT Ensemble' reaching 26.3 and a vibrant gold bar for 'Transformer (Big)' climbing to 28.4. A dashed line highlights the 2.1-point gap between them, with a 'plus 2.1' label appearing in the gap.\", narration=\"The results were immediate and shocking. On the standard English-to-German translation task, the 'Big' Transformer model didn't just win; it blew past the previous record-holders by over two full points. In the world of translation metrics, that is a massive leap forward.\"), Scene(scene_type='mid', title='Efficiency: The Cost of Brilliance', visual_description=\"The bar chart shrinks to the top left. On the right, two piles of golden coins appear. The pile for 'GNMT' is massive, towering off-screen. The pile for 'Transformer' is a tiny fraction—about one-tenth the size. Text appears: 'Training Cost: 2.3 times 10 to the 19th floating point operations'. The number for GNMT is shown to be significantly higher.\", narration=\"But the real shock wasn't just the quality—it was the price tag. The Transformer achieved these record-breaking results while using only a small fraction of the computing power required by its predecessors. It was both the smartest and the most efficient student in the room.\"), Scene(scene_type='mid', title='Proving Versatility', visual_description=\"The screen clears. A sentence appears: 'The quick brown fox jumps over the lazy dog.' Below it, the sentence transforms into a complex, branching tree structure, representing a 'Constituency Parse Tree'. The branches are drawn in glowing blue lines. A label 'English Constituency Parsing' fades in.\", narration='Critics wondered if this was just a translation trick. To prove them wrong, the researchers tested it on structural parsing—essentially diagramming the grammar of a sentence. The Transformer excelled there too, proving it had a deep, fundamental understanding of how language is built.'), Scene(scene_type='mid', title='The Death of Recurrence', visual_description=\"A long, winding chain of 'RNN' blocks stretches across the screen. A large, red 'X' mark stamps over the chain. The chain shatters into dust, and in its place, a parallel grid of 'Attention Layers' fades in. The words 'The Era of RNNs' transform into 'The Era of Transformers'.\", narration=\"This paper effectively ended the 'Era of Recurrent Networks.' It proved that recurrence—the idea of processing words one after another—wasn't just slow; it was unnecessary. The door was now wide open for parallel processing at a massive scale.\"), Scene(scene_type='mid', title='The Explosion of LLMs', visual_description=\"A horizontal timeline appears starting at 2017. At the 2017 mark, the 'Attention' paper sits as the root. In 2018, a node labeled 'BERT' branches out. In 2019, 'GPT-2' branches out in the opposite direction. By 2020 and beyond, the tree explodes into dozens of branches labeled 'GPT-3', 'T5', and 'Llama'.\", narration='Within just two years, the seeds planted by this architecture blossomed into the giants we know today. BERT and GPT-2 were direct descendants, using these same attention mechanisms to reach human-level performance on a huge variety of tasks.'), Scene(scene_type='mid', title='Modern Benchmarks', visual_description=\"The timeline fades. A new chart appears showing 'MMLU' and 'Human-Aligned' metrics. The old 'BLEU' score icon from earlier becomes a small, historical artifact in a museum display case. The background glows with a network of nodes representing modern Large Language Models.\", narration=\"Today, we've moved past simple translation scores to more complex benchmarks like MMLU, which test general knowledge and reasoning. While our metrics have evolved, the underlying engine—the Transformer—remains the gold standard for almost every modern AI.\"), Scene(scene_type='closing', title='A Monument of Research', visual_description=\"The screen displays a stylized image of the original paper's title page: 'Attention Is All You Need'. A 'Citation Count' counter starts at zero and rapidly spins up to a massive number in the tens of thousands. The paper icon turns into a golden monument as the camera zooms out to show it supporting the entire structure of modern AI.\", narration=\"With tens of thousands of citations, it is now the most influential paper in modern machine learning. It wasn't just a better way to translate text; it was the spark that ignited the current AI revolution. And it all started with the simple idea that attention is all you need.\")])}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "storyboards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "467bb84f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled Dot-Product Attention: The Mathematical Core\n",
      "0 The Ambiguity Problem\n",
      "1 Defining Q, K, and V\n",
      "2 The Dot Product Comparison\n",
      "3 The Dimension Explosion\n",
      "4 The Softmax Wall\n",
      "5 The Scaling Factor\n",
      "6 Softmax to Weights\n",
      "7 The Final Weighted Sum\n",
      "8 Hardware Optimization: FlashAttention\n",
      "9 The Complete Formula\n"
     ]
    }
   ],
   "source": [
    "test_storyboard_1 = storyboards[\"Scaled Dot-Product Attention: The Mathematical Core\"]\n",
    "print(test_storyboard_1.topic_name)\n",
    "for i, scene in enumerate(test_storyboard_1.scenes):\n",
    "    print(i, scene.title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f689c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Be Patient! This takes a while (5 minutes if you're lucky with no iterations, sometimes longer)\n",
    "storyboard_1_animation_results = eduly_animation_client.animate_single(\n",
    "    breakdown=breakdown_obj,\n",
    "    storyboard=test_storyboard_1,\n",
    "    topic_index=2,\n",
    "    max_iterations=5,\n",
    "    ratelimit=60\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28cba3b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success :  True\n",
      "/agent_workspace/rendered_videos/Scaled_Dot-Product_Attention_The_Mathematical_Core_2.mp4\n"
     ]
    }
   ],
   "source": [
    "print(\"Success : \", storyboard_1_animation_results.success)\n",
    "print(storyboard_1_animation_results.video_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b87ea75f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-Head Attention: Seeing in Parallel\n",
      "0 The Problem of the Weighted Average\n",
      "1 Enter the Multi-Head Prism\n",
      "2 Parallel Perspectives: Syntax vs. Semantics\n",
      "3 The Math of Scaling Down\n",
      "4 The Concatenation Step\n",
      "5 The Final Projection\n",
      "6 Optimizing for Speed: GQA and MQA\n",
      "7 The Power of Many Eyes\n"
     ]
    }
   ],
   "source": [
    "test_storyboard_2 = storyboards[\"Multi-Head Attention: Seeing in Parallel\"]\n",
    "print(test_storyboard_2.topic_name)\n",
    "for i, scene in enumerate(test_storyboard_2.scenes):\n",
    "    print(i, scene.title)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
